{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeb81147",
   "metadata": {},
   "source": [
    "# Partie 2 : DÃ©ploiement et quantization avec ONNX\n",
    "\n",
    "### 1. Entrainement d'un CNN simple avec PyTorch et sur le dataset Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bc1f07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6821a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# We define a simple convolutional neural network for the Fashion-MNIST dataset\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = CNN()\n",
    "print(net)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d22ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 60000\n",
      "Number of test samples: 10000\n",
      "Sample label: 9\n",
      "Dataset classes: ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the Fashion-MNIST dataset with transforms\n",
    "train_dataset = FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of test samples:\", len(test_dataset))\n",
    "print(\"Sample label:\", train_dataset[0][1])\n",
    "print(\"Dataset classes:\", train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "507836a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1, 100] loss: 0.385\n",
      "Epoch [1, 200] loss: 0.342\n",
      "Epoch [1, 300] loss: 0.334\n",
      "Epoch [1, 400] loss: 0.299\n",
      "Epoch [1, 500] loss: 0.301\n",
      "Epoch [1, 600] loss: 0.305\n",
      "Epoch [1, 700] loss: 0.290\n",
      "Epoch [1, 800] loss: 0.287\n",
      "Epoch [1, 900] loss: 0.272\n",
      "Epoch [2, 100] loss: 0.270\n",
      "Epoch [2, 200] loss: 0.254\n",
      "Epoch [2, 300] loss: 0.242\n",
      "Epoch [2, 400] loss: 0.238\n",
      "Epoch [2, 500] loss: 0.263\n",
      "Epoch [2, 600] loss: 0.243\n",
      "Epoch [2, 700] loss: 0.234\n",
      "Epoch [2, 800] loss: 0.239\n",
      "Epoch [2, 900] loss: 0.235\n",
      "Epoch [3, 100] loss: 0.204\n",
      "Epoch [3, 200] loss: 0.202\n",
      "Epoch [3, 300] loss: 0.213\n",
      "Epoch [3, 400] loss: 0.215\n",
      "Epoch [3, 500] loss: 0.214\n",
      "Epoch [3, 600] loss: 0.199\n",
      "Epoch [3, 700] loss: 0.201\n",
      "Epoch [3, 800] loss: 0.216\n",
      "Epoch [3, 900] loss: 0.214\n",
      "Epoch [4, 100] loss: 0.175\n",
      "Epoch [4, 200] loss: 0.179\n",
      "Epoch [4, 300] loss: 0.177\n",
      "Epoch [4, 400] loss: 0.179\n",
      "Epoch [4, 500] loss: 0.191\n",
      "Epoch [4, 600] loss: 0.190\n",
      "Epoch [4, 700] loss: 0.170\n",
      "Epoch [4, 800] loss: 0.183\n",
      "Epoch [4, 900] loss: 0.186\n",
      "Epoch [5, 100] loss: 0.148\n",
      "Epoch [5, 200] loss: 0.151\n",
      "Epoch [5, 300] loss: 0.165\n",
      "Epoch [5, 400] loss: 0.151\n",
      "Epoch [5, 500] loss: 0.148\n",
      "Epoch [5, 600] loss: 0.156\n",
      "Epoch [5, 700] loss: 0.170\n",
      "Epoch [5, 800] loss: 0.163\n",
      "Epoch [5, 900] loss: 0.163\n",
      "Epoch [6, 100] loss: 0.129\n",
      "Epoch [6, 200] loss: 0.126\n",
      "Epoch [6, 300] loss: 0.136\n",
      "Epoch [6, 400] loss: 0.136\n",
      "Epoch [6, 500] loss: 0.130\n",
      "Epoch [6, 600] loss: 0.132\n",
      "Epoch [6, 700] loss: 0.132\n",
      "Epoch [6, 800] loss: 0.140\n",
      "Epoch [6, 900] loss: 0.148\n",
      "Epoch [7, 100] loss: 0.099\n",
      "Epoch [7, 200] loss: 0.121\n",
      "Epoch [7, 300] loss: 0.107\n",
      "Epoch [7, 400] loss: 0.113\n",
      "Epoch [7, 500] loss: 0.107\n",
      "Epoch [7, 600] loss: 0.117\n",
      "Epoch [7, 700] loss: 0.131\n",
      "Epoch [7, 800] loss: 0.122\n",
      "Epoch [7, 900] loss: 0.113\n",
      "Epoch [8, 100] loss: 0.089\n",
      "Epoch [8, 200] loss: 0.093\n",
      "Epoch [8, 300] loss: 0.085\n",
      "Epoch [8, 400] loss: 0.100\n",
      "Epoch [8, 500] loss: 0.090\n",
      "Epoch [8, 600] loss: 0.096\n",
      "Epoch [8, 700] loss: 0.108\n",
      "Epoch [8, 800] loss: 0.100\n",
      "Epoch [8, 900] loss: 0.108\n",
      "Epoch [9, 100] loss: 0.075\n",
      "Epoch [9, 200] loss: 0.071\n",
      "Epoch [9, 300] loss: 0.074\n",
      "Epoch [9, 400] loss: 0.080\n",
      "Epoch [9, 500] loss: 0.078\n",
      "Epoch [9, 600] loss: 0.076\n",
      "Epoch [9, 700] loss: 0.082\n",
      "Epoch [9, 800] loss: 0.084\n",
      "Epoch [9, 900] loss: 0.088\n",
      "Epoch [10, 100] loss: 0.061\n",
      "Epoch [10, 200] loss: 0.063\n",
      "Epoch [10, 300] loss: 0.053\n",
      "Epoch [10, 400] loss: 0.072\n",
      "Epoch [10, 500] loss: 0.081\n",
      "Epoch [10, 600] loss: 0.069\n",
      "Epoch [10, 700] loss: 0.074\n",
      "Epoch [10, 800] loss: 0.071\n",
      "Epoch [10, 900] loss: 0.078\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAdDpJREFUeJzt3Xlc1NXeB/DPzMDMsO87KIsLuaKguGtJYlm5VWqLSt2617Sb2Wo9ajtq1jXT9LZqtmh21cqUUlLLxA3FXVQEWYdVGNYZmPk9fwwzOgIKODID83m/XvN65Pc7vzPfcXzie8/5nnNEgiAIICIiIrIiYnMHQERERNTWmAARERGR1WECRERERFaHCRARERFZHSZAREREZHWYABEREZHVYQJEREREVocJEBEREVkdJkBERERkdZgAEXVAM2fORHBwcKuefeONNyASiUwbEFm0W/n3QtReMQEiakMikahZrz179pg7VLOYOXMmHB0dzR1Gh8B/a0Q3JuJZYERt55tvvjH6+euvv8bOnTuxfv16o+t33303fHx8Wv0+tbW10Gq1kMlkLX62rq4OdXV1kMvlrX7/1po5cyZ+/PFHVFRUtPl7dzQt+bfm7u7e6n8vRO0VEyAiM5ozZw5WrVqFm/2/YVVVFezt7dsoKvNhAtRylZWVcHBwuGm75v5bI7IWnAIjsjCjRo1Cr169kJycjBEjRsDe3h6vvfYaAOCnn37CuHHj4O/vD5lMhrCwMLz99tvQaDRGfVxf05GRkQGRSIRly5bh008/RVhYGGQyGQYMGIDDhw8bPdtYDZBIJMKcOXOwdetW9OrVCzKZDD179kRCQkKD+Pfs2YOoqCjI5XKEhYXhv//9r8nrijZt2oTIyEjY2dnB09MTjz32GHJycozaKBQKxMXFITAwEDKZDH5+fhg/fjwyMjIMbY4cOYLY2Fh4enrCzs4OISEheOKJJ5oVwyeffIKePXtCJpPB398fs2fPRmlpqeH+nDlz4OjoiKqqqgbPTps2Db6+vkbf244dOzB8+HA4ODjAyckJ48aNw+nTp42e008RpqWl4d5774WTkxMeffTRZsV7Izf697Jq1SqEhobC3t4eY8aMQVZWFgRBwNtvv43AwEDY2dlh/PjxKCkpadBvcz4TkbnYmDsAImqouLgY99xzD6ZOnYrHHnvMMB22du1aODo6Yt68eXB0dMQff/yBhQsXQqlU4v33379pv9999x3Ky8vxz3/+EyKRCEuXLsWkSZNw6dIl2Nra3vDZffv2YfPmzXjmmWfg5OSEFStWYPLkycjMzISHhwcA4NixYxg7diz8/Pzw5ptvQqPR4K233oKXl9et/6XUW7t2LeLi4jBgwADEx8cjPz8fH330Ef7++28cO3YMrq6uAIDJkyfj9OnTePbZZxEcHIyCggLs3LkTmZmZhp/HjBkDLy8vvPrqq3B1dUVGRgY2b9580xjeeOMNvPnmm4iJicGsWbOQmpqK1atX4/Dhw/j7779ha2uLKVOmYNWqVfj111/x0EMPGZ6tqqrCL7/8gpkzZ0IikQAA1q9fjxkzZiA2NhZLlixBVVUVVq9ejWHDhuHYsWNGyUldXR1iY2MxbNgwLFu27LaODH777bdQq9V49tlnUVJSgqVLl+Lhhx/GXXfdhT179uCVV17BxYsX8fHHH+PFF1/El19+aXi2JZ+JyCwEIjKb2bNnC9f/v+HIkSMFAMKaNWsatK+qqmpw7Z///Kdgb28v1NTUGK7NmDFD6Ny5s+Hn9PR0AYDg4eEhlJSUGK7/9NNPAgDhl19+MVxbtGhRg5gACFKpVLh48aLh2vHjxwUAwscff2y4dv/99wv29vZCTk6O4dqFCxcEGxubBn02ZsaMGYKDg0OT99VqteDt7S306tVLqK6uNlzftm2bAEBYuHChIAiCcOXKFQGA8P777zfZ15YtWwQAwuHDh28a17UKCgoEqVQqjBkzRtBoNIbrK1euFAAIX375pSAIgqDVaoWAgABh8uTJRs//8MMPAgDhzz//FARBEMrLywVXV1fhqaeeMmqnUCgEFxcXo+szZswQAAivvvpqi2IWhMb/rV3bb2P/Xry8vITS0lLD9fnz5wsAhL59+wq1tbWG69OmTROkUqnh32BLPhORuXAKjMgCyWQyxMXFNbhuZ2dn+HN5eTmKioowfPhwVFVV4dy5czftd8qUKXBzczP8PHz4cADApUuXbvpsTEwMwsLCDD/36dMHzs7Ohmc1Gg127dqFCRMmwN/f39CuS5cuuOeee27af3McOXIEBQUFeOaZZ4yKtMeNG4fw8HD8+uuvAHR/T1KpFHv27MGVK1ca7Us/UrRt2zbU1tY2O4Zdu3ZBrVZj7ty5EIuv/if0qaeegrOzsyEGkUiEhx56CNu3bzeqadq4cSMCAgIwbNgwAMDOnTtRWlqKadOmoaioyPCSSCSIjo7G7t27G8Qwa9asZsd7Kx566CG4uLgYfo6OjgYAPPbYY7CxsTG6rlarDdOQrflMRG2NCRCRBQoICIBUKm1w/fTp05g4cSJcXFzg7OwMLy8vPPbYYwCAsrKym/bbqVMno5/1yVBTScKNntU/r3+2oKAA1dXV6NKlS4N2jV1rjcuXLwMAunfv3uBeeHi44b5MJsOSJUuwY8cO+Pj4YMSIEVi6dCkUCoWh/ciRIzF58mS8+eab8PT0xPjx4/HVV19BpVK1KgapVIrQ0FDDfUCXcFZXV+Pnn38GAFRUVGD79u146KGHDDVRFy5cAADcdddd8PLyMnr9/vvvKCgoMHofGxsbBAYG3vwvywSu/871yVBQUFCj1/X/Flr6mYjMgTVARBbo2pEevdLSUowcORLOzs546623EBYWBrlcjqNHj+KVV16BVqu9ab/6mpPrCc1YGXQrz5rD3Llzcf/992Pr1q347bffsGDBAsTHx+OPP/5Av379IBKJ8OOPP+LAgQP45Zdf8Ntvv+GJJ57ABx98gAMHDphkP6JBgwYhODgYP/zwAx555BH88ssvqK6uxpQpUwxt9N/b+vXr4evr26CPa0daAF1yd+3I0+3U1Hd+s38LLf1MRObAf4VE7cSePXtQXFyMzZs3Y8SIEYbr6enpZozqKm9vb8jlcly8eLHBvcautUbnzp0BAKmpqbjrrruM7qWmphru64WFheGFF17ACy+8gAsXLiAiIgIffPCB0R45gwYNwqBBg/Duu+/iu+++w6OPPooNGzbgH//4x01jCA0NNVxXq9VIT09HTEyMUfuHH34YH330EZRKJTZu3Ijg4GAMGjTIKEZA9/d3/bPtVUf8TNTxcAqMqJ3Q/6/ua0dc1Go1PvnkE3OFZEQikSAmJgZbt25Fbm6u4frFixexY8cOk7xHVFQUvL29sWbNGqOpqh07duDs2bMYN24cAN1Kq5qaGqNnw8LC4OTkZHjuypUrDUavIiIiAOCG02AxMTGQSqVYsWKF0fNffPEFysrKDDHoTZkyBSqVCuvWrUNCQgIefvhho/uxsbFwdnbGe++912gtUmFhYZOxWKqO+Jmo4+EIEFE7MWTIELi5uWHGjBn497//DZFIhPXr11vUFNQbb7yB33//HUOHDsWsWbOg0WiwcuVK9OrVCykpKc3qo7a2Fu+8806D6+7u7njmmWewZMkSxMXFYeTIkZg2bZphGXxwcDCef/55AMD58+cxevRoPPzww+jRowdsbGywZcsW5OfnY+rUqQCAdevW4ZNPPsHEiRMRFhaG8vJyfPbZZ3B2dsa9997bZHxeXl6YP38+3nzzTYwdOxYPPPAAUlNT8cknn2DAgAGGmiy9/v37o0uXLnj99dehUqmMpr8AwNnZGatXr8bjjz+O/v37Y+rUqfDy8kJmZiZ+/fVXDB06FCtXrmzW352l6IifiToeJkBE7YSHhwe2bduGF154Af/3f/8HNzc3PPbYYxg9ejRiY2PNHR4AIDIyEjt27MCLL76IBQsWICgoCG+99RbOnj3brFVqgG5Ua8GCBQ2uh4WF4ZlnnsHMmTNhb2+PxYsX45VXXoGDgwMmTpyIJUuWGFZ2BQUFYdq0aUhMTMT69ethY2OD8PBw/PDDD5g8eTIAXRH0oUOHsGHDBuTn58PFxQUDBw7Et99+i5CQkBvG+MYbb8DLywsrV67E888/D3d3dzz99NN47733Gt1PacqUKXj33XfRpUsX9O/fv8H9Rx55BP7+/li8eDHef/99qFQqBAQEYPjw4Y2uBmwPOuJnoo6FR2EQ0W03YcIEnD592rA6iIjI3FgDREQmVV1dbfTzhQsXsH37dowaNco8ARERNYIjQERkUn5+fpg5c6ZhT5zVq1dDpVLh2LFj6Nq1q7nDIyICwBogIjKxsWPH4vvvv4dCoYBMJsPgwYPx3nvvMfkhIovCESAiIiKyOqwBIiIiIqtjEQnQqlWrEBwcDLlcjujoaBw6dKhZz23YsAEikQgTJkwwui4IAhYuXAg/Pz/Y2dkhJiaGq0+IiIjIwOxTYBs3bsT06dOxZs0aREdHY/ny5di0aRNSU1Ph7e3d5HMZGRkYNmwYQkND4e7ujq1btxruLVmyBPHx8Vi3bh1CQkKwYMECnDx5EmfOnDE6QbopWq0Wubm5cHJyMhxYSERERJZNEASUl5fD39//5mfmCWY2cOBAYfbs2YafNRqN4O/vL8THxzf5TF1dnTBkyBDh888/F2bMmCGMHz/ecE+r1Qq+vr7C+++/b7hWWloqyGQy4fvvv29WTFlZWQIAvvjiiy+++OKrHb6ysrJu+rverKvA1Go1kpOTMX/+fMM1sViMmJgYJCUlNfncW2+9BW9vbzz55JP466+/jO6lp6dDoVAYHcDn4uKC6OhoJCUlGbbBv5ZKpTI6+0eoHxTLysqCs7Nzqz8fERERtR2lUomgoCA4OTndtK1ZE6CioiJoNBr4+PgYXffx8Wly2/x9+/bhiy++aPJcIYVCYejj+j71964XHx+PN998s8F1Z2dnJkBERETtTHPKVyyiCLq5ysvL8fjjj+Ozzz6Dp6enyfqdP38+ysrKDK+srCyT9U1ERESWx6wjQJ6enpBIJMjPzze6np+fD19f3wbt09LSkJGRgfvvv99wTavVAgBsbGyQmppqeC4/Px9+fn5GfUZERDQah0wmg0wmu9WPQ0RERO2EWUeApFIpIiMjkZiYaLim1WqRmJiIwYMHN2gfHh6OkydPIiUlxfB64IEHcOeddyIlJQVBQUEICQmBr6+vUZ9KpRIHDx5stE8iIiKyPmY/CmPevHmYMWMGoqKiMHDgQCxfvhyVlZWIi4sDAEyfPh0BAQGIj4+HXC5Hr169jJ53dXUFAKPrc+fOxTvvvIOuXbsalsH7+/s32C+IiIiIrJPZE6ApU6agsLAQCxcuhEKhQEREBBISEgxFzJmZmTdfy3+dl19+GZWVlXj66adRWlqKYcOGISEhoVl7ABEREVHHZ/aNEC2RUqmEi4sLysrKuAqMiIionWjJ7+92tQqMiIiIyBSYABEREZHVYQJEREREVocJEBEREVkdJkBERERkdZgAERERkdUx+z5A1qRWo0W+sgY2YjF8XbgnERERkblwBKgNfbjzPIYt2Y01e9PMHQoREZFVYwLUhvzrR31ySqvNHAkREZF1YwLUhvxc7AAAeWVMgIiIiMyJCVAb8netT4BKa8wcCRERkXVjAtSG/F11U2DFlWrU1GrMHA0REZH1YgLUhlzsbGFnKwEA5JVxFIiIiMhcmAC1IZFIZBgFymMhNBERkdkwAWpj+jogrgQjIiIyHyZAbczfsBKMU2BERETmwgSojfnpp8C4FJ6IiMhsmAC1Mf0IUA6XwhMREZkNE6A2dnUvII4AERERmQsToDamnwLLLa2GIAhmjoaIiMg6MQFqY/opsEq1BsqaOjNHQ0REZJ2YALUxO6kEbva2AFgITUREZC5MgMzAcCgqC6GJiIjMggmQGeh3g+ZmiERERObBBMgMDCvBOAVGRERkFkyAzIBTYERERObFBMgMOAVGRERkXkyAzODqFBhHgIiIiMyBCZAZ+LlcPQ9Mq+VmiERERG2NCZAZ+DjLIRYBtRoBRZUqc4dDRERkdZgAmYGtRAxvp/pRIBZCExERtTkmQGZy7ZlgRERE1LaYAJmJvhA6l4XQREREbY4JkJn46wuhOQJERETU5pgAmYl+M8Rc7gZNRETU5pgAmYlhCoxF0ERERG2OCZCZ6HeD5nlgREREbY8JkJnop8AKylVQ12nNHA0REZF1YQJkJh4OUkhtxBAEIF/JaTAiIqK2ZBEJ0KpVqxAcHAy5XI7o6GgcOnSoybabN29GVFQUXF1d4eDggIiICKxfv96ozcyZMyESiYxeY8eOvd0fo0XEYtE1R2IwASIiImpLNuYOYOPGjZg3bx7WrFmD6OhoLF++HLGxsUhNTYW3t3eD9u7u7nj99dcRHh4OqVSKbdu2IS4uDt7e3oiNjTW0Gzt2LL766ivDzzKZrE0+T0v4u9jhcnEVN0MkIiJqY2YfAfrwww/x1FNPIS4uDj169MCaNWtgb2+PL7/8stH2o0aNwsSJE3HHHXcgLCwMzz33HPr06YN9+/YZtZPJZPD19TW83Nzc2uLjtIhhN2gWQhMREbUpsyZAarUaycnJiImJMVwTi8WIiYlBUlLSTZ8XBAGJiYlITU3FiBEjjO7t2bMH3t7e6N69O2bNmoXi4mKTx3+r/OsLoXkeGBERUdsy6xRYUVERNBoNfHx8jK77+Pjg3LlzTT5XVlaGgIAAqFQqSCQSfPLJJ7j77rsN98eOHYtJkyYhJCQEaWlpeO2113DPPfcgKSkJEomkQX8qlQoq1dVT2ZVKpQk+3c1d3QuII0BERERtyew1QK3h5OSElJQUVFRUIDExEfPmzUNoaChGjRoFAJg6daqhbe/evdGnTx+EhYVhz549GD16dIP+4uPj8eabb7ZV+AZXp8A4AkRERNSWzDoF5unpCYlEgvz8fKPr+fn58PX1bfI5sViMLl26ICIiAi+88AIefPBBxMfHN9k+NDQUnp6euHjxYqP358+fj7KyMsMrKyurdR+ohQxTYKwBIiIialNmTYCkUikiIyORmJhouKbVapGYmIjBgwc3ux+tVms0hXW97OxsFBcXw8/Pr9H7MpkMzs7ORq+2oN8NurSqFlXqujZ5TyIiIrKAKbB58+ZhxowZiIqKwsCBA7F8+XJUVlYiLi4OADB9+nQEBAQYRnji4+MRFRWFsLAwqFQqbN++HevXr8fq1asBABUVFXjzzTcxefJk+Pr6Ii0tDS+//DK6dOlitEzeEjjJbeEks0G5qg65pTXo4u1o7pCIiIisgtkToClTpqCwsBALFy6EQqFAREQEEhISDIXRmZmZEIuvDlRVVlbimWeeQXZ2Nuzs7BAeHo5vvvkGU6ZMAQBIJBKcOHEC69atQ2lpKfz9/TFmzBi8/fbbFrkXkJ+rHOX5Fcgrq2YCRERE1EZEgiAI5g7C0iiVSri4uKCsrOy2T4fN/OoQ9qQWYsnk3pgyoNNtfS8iIqKOrCW/v82+EaK10x+Kmsu9gIiIiNoMEyAzC6gvhM4qqTJzJERERNaDCZCZ9QpwAQD8ciIX5xRtswEjERGRtWMCZGYju3kh5g4f1GoEvLjpOGo1WnOHRERE1OExATIzkUiE9yb1gqu9LU7lKLF6T5q5QyIiIurwmABZAG8nOd58oCcAYEXiBZzOLTNzRERERB0bEyAL8UBff8T29EGdVsCLm05AXcepMCIiotuFCZCFEIlEeGdCb7jZ2+JsnhIrdzd+bhkRERHdOiZAFsTLSYa3J/QCAKzafRGncjgVRkREdDswAbIw9/Xxx7jeftBoBXy487y5wyEiIuqQmABZoJlDgwEAqYpy8wZCRETUQTEBskDBHg4AgNyyatTUaswcDRERUcfDBMgCeTpK4SizgSDwiAwiIqLbgQmQBRKJRAj2tAcApBdVmjkaIiKijocJkIXST4NlFDMBIiIiMjUmQBbqagLEKTAiIiJTYwJkoYI96xMgToERERGZHBMgCxVSXwPEBIiIiMj0mABZqM6GpfA1XApPRERkYkyALJSHgxROMhsAQCaXwhMREZkUEyALpVsKrxsF4lJ4IiIi02ICZME6e+jqgC5zKTwREZFJMQGyYCGGESBOgREREZkSEyALZtgLiFNgREREJsUEyILpj8PgFBgREZFpMQGyYMFcCk9ERHRbMAGyYO4OUjjJdUvhL/NIDCIiIpNhAmTBRCKRoRCah6ISERGZDhMgC9eZhdBEREQmxwTIwoXU7wXEESAiIiLTYQJk4a6eCs8aICIiIlNhAmThDFNgHAEiIiIyGSZAFk5fBJ1XVoNqNZfCExERmQITIAvnZm8LZzlPhSciIjIlJkAWjqfCExERmR4ToHYgmHVAREREJsUEqB3QjwDxTDAiIiLTYALUDoTUH4rKKTAiIiLTYALUDlzdDfrmRdDZV6rwxNrDiN9+9naHRURE1G4xAWoHQuoTIIXyxkvh/zxfiPs+3oc/zhXgv39eQk5pdVuFSERE1K5YRAK0atUqBAcHQy6XIzo6GocOHWqy7ebNmxEVFQVXV1c4ODggIiIC69evN2ojCAIWLlwIPz8/2NnZISYmBhcuXLjdH+O2cXOQwsXOFgBwuaThNJhWK2DlHxcw46tDKK2qNVz/9URum8VIRETUnpg9Adq4cSPmzZuHRYsW4ejRo+jbty9iY2NRUFDQaHt3d3e8/vrrSEpKwokTJxAXF4e4uDj89ttvhjZLly7FihUrsGbNGhw8eBAODg6IjY1FTU1NW30skwvWnwl2XR1QWXUtnl5/BMt+Pw9BAKYNDMKC+3oAAH45ntfmcRIREbUHIkEQBHMGEB0djQEDBmDlypUAAK1Wi6CgIDz77LN49dVXm9VH//79MW7cOLz99tsQBAH+/v544YUX8OKLLwIAysrK4OPjg7Vr12Lq1Kk37U+pVMLFxQVlZWVwdnZu/Yczoec2HMNPKbl49Z5w/GtkGPLKqrH5aA6+O5iJnNJqSG3EeHt8T0wZ0AlFFSpEv5cIjVbAnhdHGVaRERERdWQt+f1t1hEgtVqN5ORkxMTEGK6JxWLExMQgKSnpps8LgoDExESkpqZixIgRAID09HQoFAqjPl1cXBAdHd1knyqVCkql0uhlafR7Af1+WoHHPj+IIYv/wPu/pSKntBoBrnb437+GYMqATgAAT0cZhoR5AAB+PclRICIiouvZmPPNi4qKoNFo4OPjY3Tdx8cH586da/K5srIyBAQEQKVSQSKR4JNPPsHdd98NAFAoFIY+ru9Tf+968fHxePPNN2/lo9x2wfVL4Y9mlhquRYe4Y3JkIMb19oODzPirvK+PH/66UIRfjudi9p1d2jJUIiIii2fWBKi1nJyckJKSgoqKCiQmJmLevHkIDQ3FqFGjWtXf/PnzMW/ePMPPSqUSQUFBJorWNIaGecLTUQZ7qQST+gdgcv9ABLnbN9k+tqcv/m/rKZxTlONCfjm6+ji1YbRERESWzawJkKenJyQSCfLz842u5+fnw9fXt8nnxGIxunTRjWpERETg7NmziI+Px6hRowzP5efnw8/Pz6jPiIiIRvuTyWSQyWS3+GluL29nOY78XwwEQYBIJLppe1d7KYZ39cIf5wrwy4k8zLubCRAREZGeWWuApFIpIiMjkZiYaLim1WqRmJiIwYMHN7sfrVYLlUoFAAgJCYGvr69Rn0qlEgcPHmxRn5aqOcmP3v19dQngthO5MHOtOxERkUUx+xTYvHnzMGPGDERFRWHgwIFYvnw5KisrERcXBwCYPn06AgICEB8fD0BXrxMVFYWwsDCoVCps374d69evx+rVqwHoEoS5c+finXfeQdeuXRESEoIFCxbA398fEyZMMNfHNIuYO3wgtRHjUmElzuaVo4e/ZaxoIyIiMjezJ0BTpkxBYWEhFi5cCIVCgYiICCQkJBiKmDMzMyEWXx2oqqysxDPPPIPs7GzY2dkhPDwc33zzDaZMmWJo8/LLL6OyshJPP/00SktLMWzYMCQkJEAul7f55zMnJ7kt7uzuhd9O5+OXE7lMgIiIiOqZfR8gS2SJ+wC11rYTuZjz3TEEudvhz5fubNEUGhERUXvSbvYBotvvrnBv2NlKkFVSjePZZeYOh4iIyCIwAerg7KU2iOmhm07cdpxngxEREQFMgKzCfX10q8H+dzQbu881fsYaERGRNWECZAVGdvNCmJcDrlTVIm7tYTyx9nCDQ1WJiIisCRMgKyC3lWDr7KF4ekQobMQi/HGuAGP+8yeWJJxDparO3OERERG1OSZAVsJJbovX7r0DCXNHYEQ3L6g1Wqzek4YJq/6Gqk5j7vCIiIjaFBMgK9PF2xHr4gbg8+lRcLW3xYWCCvx9scjcYREREbUpJkBWSCQSIaaHD8b39QcA7DipMHNEREREbYsJkBUb20u3Omzn2XzUarRmjoaIiKjtMAGyYgND3OHhIEVpVS0OXioxdzhERERthgmQFZOIRRjTU7dJ4o5TeWaOhoiIqO0wAbJysT19AQC/nc6HRstj4YiIyDowAbJyQ8I84SS3QVGFCsmXr5g7HCIiojbBBMjKSW3EuPsOToMREZF1YQJEGNurfhrslAKCwGkwIiLq+JgAEUZ084K9VILcshoczy4zdzhERES3HRMggtxWgjvDvQFwGoyIiKwDEyACANxTPw2WwGkwIiKyAkyACABwZ3dvyGzEuFxchbN55eYOh4iI6LZiAkQAAAeZDUZ08wIAJHAajIiIOjgmQGSgnwbbdjIPheUqM0dDRER0+9iYOwCyHKPv8IGtRIRLhZUY8O4udPV2xOAwDwwO9cDgMA+42kvNHSIREZFJiARWvDagVCrh4uKCsrIyODs7mzucNvX9oUx8nXQZZ/OURtcdZTbYOW8E/FzszBQZERHRjbXk9zdHgMjItIGdMG1gJ5RUqnHwUjGSLhVj+0kFiipU+PVEHv4xPNTcIRIREd0y1gBRo9wdpLintx/eGt8Ls+8MAwD8fjrfzFERERGZBhMguqkx9SfGH7lcgqIKFkcTEVH7xwSIbirA1Q69ApyhFYDEsxwFIiKi9o8JEDVLbA/dKBCnwYiIqCNgAkTNop8G++tiESpUdWaOhoiI6NYwAaJm6ebjiGAPe6jrtPjzfOEt9VVWXYufUnJQq9GaKDoiIqKWYQJEzSISiQyjQL+dVtxSXx/+nornNqTgi33ppgiNiIioxZgAUbPF9vQBAPxxrgDqutaP3hzKuAIA2JNaYJK4iIiIWooJEDVbRJAbPB1lKK+pw4FLxa3qo6ZWgwv5utPmj14uRU2txpQhEhERNQsTIGo2iViEu3t4AwB+P9O6abBURTnqtLrTV9QaLZIvXzFZfERERM3FBIhaRF8H9PvpfGi1LT9G7mROmdHP+9OKTBIXERFRSzABohYZEuYBR5kNCspVOJ5d2uLnT9UnQAGuukNV96e1biqNiIjoVjABohaR2UgwqrsXAOC3VmyKqB8BihsaDAA4kV2G8ppak8VHRETUHEyAqMUM02AtrANS1Wlwvr4AOranL4I97KHRCjicUWLyGImIiG6ECRC12J3dvWArEeFSYaVhRVdznFdUoFYjwMXOFoFudhgc5gEA2H+R02BERNS2LCIBWrVqFYKDgyGXyxEdHY1Dhw412fazzz7D8OHD4ebmBjc3N8TExDRoP3PmTIhEIqPX2LFjb/fHsBpOcluM6KqbBvsxObvZz+mnv3oHuEAkEmFwmCcA1gEREVHbM3sCtHHjRsybNw+LFi3C0aNH0bdvX8TGxqKgoPFN8vbs2YNp06Zh9+7dSEpKQlBQEMaMGYOcnByjdmPHjkVeXp7h9f3337fFx7EaUwYEAdAlQM3dFFGfAPUKcAEADA7VjQCdyVPiSqX6NkRJRETUOLMnQB9++CGeeuopxMXFoUePHlizZg3s7e3x5ZdfNtr+22+/xTPPPIOIiAiEh4fj888/h1arRWJiolE7mUwGX19fw8vNza0tPo7VuCvcG95OMhRXqrHzTPOKoU9dMwIEAF5OMnTzcQSAVm+sSERE1BpmTYDUajWSk5MRExNjuCYWixETE4OkpKRm9VFVVYXa2lq4u7sbXd+zZw+8vb3RvXt3zJo1C8XF/AVrSjYSMR6O0o0CbTicedP26jotUhW6eqFeAc6G60M4DUZERGZg1gSoqKgIGo0GPj4+Rtd9fHygUDRvhdErr7wCf39/oyRq7Nix+Prrr5GYmIglS5Zg7969uOeee6DRNH7sgkqlglKpNHrRzemnwf66UISskqobtj2fXw61RgtnuQ06udsbrhsKobkhIhERtSGzT4HdisWLF2PDhg3YsmUL5HK54frUqVPxwAMPoHfv3pgwYQK2bduGw4cPY8+ePY32Ex8fDxcXF8MrKCiojT5B+xbkbo/hXXUjODcbBTp1Tf2PSCQyXB8U4gGRCEgrrES+sub2BUtERHQNsyZAnp6ekEgkyM83riHJz8+Hr6/vDZ9dtmwZFi9ejN9//x19+vS5YdvQ0FB4enri4sWLjd6fP38+ysrKDK+srKyWfRArNm1gJwDApiPZqNM0XQx98rr6Hz0Xe1v08tddS+I0GBERtRGzJkBSqRSRkZFGBcz6gubBgwc3+dzSpUvx9ttvIyEhAVFRUTd9n+zsbBQXF8PPz6/R+zKZDM7OzkYvap6YO3zg4SBFQbkKf5xrfOUeAJzK1U0r9rouAQJ0x2sAnAYjIqK2Y/YpsHnz5uGzzz7DunXrcPbsWcyaNQuVlZWIi4sDAEyfPh3z5883tF+yZAkWLFiAL7/8EsHBwVAoFFAoFKioqAAAVFRU4KWXXsKBAweQkZGBxMREjB8/Hl26dEFsbKxZPmNHJrUR48HIQADA94canwar1WhxNq/pBOhqHRBHgIiIqG2YPQGaMmUKli1bhoULFyIiIgIpKSlISEgwFEZnZmYiLy/P0H716tVQq9V48MEH4efnZ3gtW7YMACCRSHDixAk88MAD6NatG5588klERkbir7/+gkwmM8tn7Oj0xdB7zxcit7S6wf0L+RVQ12nhJLNB52sKoPUGBLvDRixC9pXqmxZTExERmYKNuQMAgDlz5mDOnDmN3ru+cDkjI+OGfdnZ2eG3334zUWTUHKFejhgU6o4Dl0rww5EszI3pZnRfXwDdM8AZYrGowfMOMhtEBLniyOUr2HexyFBXREREdLuYfQSIOgZ90vLD4SxotILRvVO5jRdAX2tEN93RGmv/zmjwPBERkakxASKTiO3pC1d7W+SW1WDZ76nQXpPEXH8ERmOmD+4MZ7kNUvPLsflo888XIyIiag0mQGQSclsJ/n1XVwDA6j1peHbDMdTUalB3kwJoPVd7KWbf2QUA8OHO86ipbXzTSiIiIlNgAkQm88SwECx9sA9sJSL8eiIPUz49gAOXSlBTq4WjzAYhHg43fH7GkGD4u8iRV1aDr/7OaLRNgbIG720/iyMZJbfhExARkbVgAkQm9XBUENY/GQ1Xe1sczypF3NpDAIAe/o0XQF9LbivBC2O6AwA+2XOxwQnxBeU1mPrpAXz65yVM++wAfj6ee3s+BBERdXhMgMjkBoV6YMszQxHq6YBaja4W6EYF0Nea0C8A4b5OKK+pw6rdV3fuLqpQ4ZHPDuJSUSWkEjFqNQL+/f0xfP7XpdvyGYiIqGNjAkS3RYinAzY/M8Swy/Oo7l7Nek4iFuHVe8IBAF8nXUZWSRVKKtV47PODuFhQAV9nOX57fgTihgYDAN759Sze2XbGqOiaiIjoZkSCIPA3x3WUSiVcXFxQVlbGYzFukSAIKKlUw8Ox+ZtQCoKARz8/iP1pxRjTwwfZV6pxJk8JbycZNv5zMEI8HSAIAj798xLid5wDADzQ1x/LHuoLqQ1zeiIia9WS39/8bUG3lUgkalHyo39m/j13AAB+P5OPM3lKeDrK8N1TgxDi6WBo88+RYfjPlL6wEYvw8/FcvPvrGZPHT0REHRMTILJIvQNd8EBffwCAh4MU3z8VjS7ejg3aTewXiPcf6gMA+PMCD1MlIqLmsYijMIga887EXujh74yxPX0R7Nn0EvqhYZ4AgMvFlaip1UBuK2mrEImIqJ3iCBBZLGe5Lf41MuyGyQ8AeDnJ4Cy3gVYA0osq2yg6IiJqz5gAUbsnEonQzccJAHChoMLM0RARUXvQqgQoKysL2dlXz2s6dOgQ5s6di08//dRkgRG1RFcfXX3QxfxyM0dCRETtQasSoEceeQS7d+8GACgUCtx99904dOgQXn/9dbz11lsmDZCoObp4cwSIiIiar1UJ0KlTpzBw4EAAwA8//IBevXph//79+Pbbb7F27VpTxkfULF3rV4gxASIiouZoVQJUW1sLmUy3t8uuXbvwwAMPAADCw8ORl5dnuuiImkk/BZZRVAl1ndbM0RARkaVrVQLUs2dPrFmzBn/99Rd27tyJsWPHAgByc3Ph4eFh0gCJmsPXWQ5HmQ3qtAIuF3MlGBER3VirEqAlS5bgv//9L0aNGoVp06ahb9++AICff/7ZMDVG1JZEIpFho0ROgxER0c20aiPEUaNGoaioCEqlEm5ubobrTz/9NOzt7U0WHFFLdPV2REpWKS7kVwC9zR0NERFZslaNAFVXV0OlUhmSn8uXL2P58uVITU2Ft7e3SQMkai59HdCFAi6FJyKiG2tVAjR+/Hh8/fXXAIDS0lJER0fjgw8+wIQJE7B69WqTBkjUXF3rl8Jf5BQYERHdRKsSoKNHj2L48OEAgB9//BE+Pj64fPkyvv76a6xYscKkARI1l74G6FJhJeo0XAlGRERNa1UCVFVVBScn3f/a/v333zFp0iSIxWIMGjQIly9fNmmARM0V4GoHO1sJ1BotLpdUmTscIiKyYK1KgLp06YKtW7ciKysLv/32G8aMGQMAKCgogLOzs0kDJGousfialWD5nAYjIqKmtSoBWrhwIV588UUEBwdj4MCBGDx4MADdaFC/fv1MGiBRSxjOBGMhNBER3UCrlsE/+OCDGDZsGPLy8gx7AAHA6NGjMXHiRJMFR9RSXS30TLDM4iqczCnD6Du8IbeVmDscIiKr16oECAB8fX3h6+trOBU+MDCQmyCS2XW1wCkwQRDw9PojOKcoh7+LHC+N7Y7xfQMgFovMHRoRkdVq1RSYVqvFW2+9BRcXF3Tu3BmdO3eGq6sr3n77bWi1XH1D5qOfAksrrIBGK7T4+fSiSiz7LRXDl/6BIfGJeH5jCn44koXsK60vqr5QUIFzCt2UXG5ZDZ7feBwPrNqH/ReLWt0nERHdmlaNAL3++uv44osvsHjxYgwdOhQAsG/fPrzxxhuoqanBu+++a9IgiZor0M0eMhsxVHVaZF+pQmcPh5s+U15Ti19P5OHH5GwcuXzF6N6WYznYciwHANDJ3R4T+wVgbkxXiETNH73ZflJ3QPDwrp4YEuaJT3ZfxKkcJR75/CBGh3tj+dQIOMltW/ApiYjoVrUqAVq3bh0+//xzwynwANCnTx8EBATgmWeeYQJEZiMRixDm5YgzeUpcyK+4aQJUXKHCfR/vQ15ZDQBALAJGdPPCg5GBcLOXIimtGPvTinA8uwyZJVX4KPECxvT0QU9/l2bHtOOkAgAwPiIAD0YG4uGoQKxIvIBvD2Yi8VwBdp3Nx8R+ga3/0ERE1GKtSoBKSkoQHh7e4Hp4eDhKSkpuOSiiW9HVpz4BKqhATA+fG7b98u905JXVwNdZjplDgzGxXwB8nOWG+0O7eALojgpVHWZ/exR7zxdi+8m8ZidAFwsqkJpfDhuxCHffoYvFw1GGN8f3gqpOiw2Hs3C5mHsWERG1tVbVAPXt2xcrV65scH3lypXo06fPLQdFdCsMhdA3WQqvrKnF1/t1G3e+Ob4n/jUyzCj5uZajzAaT+gcAALafVEAQmldflHBKN/01tIsnXOyNp7mC3HUHB2eVVDerLyIiMp1WjQAtXboU48aNw65duwx7ACUlJSErKwvbt283aYBELdWlmWeCrU+6jHJVHbp6OxpGZ25k9B0+kNmIkV5UiTN5ymaNAm2vn/66t7dvg3tXEyCOABERtbVWjQCNHDkS58+fx8SJE1FaWorS0lJMmjQJp0+fxvr1600dI1GLXN0MsQLaJlaCVas1+HJfOgDgmTvDmrUk3VFmgzu7ewMAfj2Rd9P2GfWJkkQswt09GkmA3OwAAFm3sMKMiIhap9X7APn7+zcodj5+/Di++OILfPrpp7ccGFFrdXa3h61EhCq1Brll1Qh0s2/QZsPhTBRXqhHkbof7+/g3u+9xffyQcFqB7Sfz8FJs9xuuBttxSjf6MzjUA+4O0gb39SNACmUNVHUayGy4QSIRUVtp1QgQkSWzkYgR6qmvA2o4Daau0+LTPy8BAP41Mgw2kub/v8Fd4d6Q2YiRUVyF07nKG7bdUV//c08j018A4OEghb1UAkEAcq6wDoiIqC0xAaIOyTAN1siO0FuP5SCvrAbeTjJM7t+y5ecOMhvcFV4/DXay6WmwrJIqnMgug1gExPZsPAESiUQIqh+dymICRETUppgAUYekPxPsfL7xSjCNVsDqvWkAgKeGh7bqXK5xffwA6OqAmloNllA//RUd4gFPR1mTfemnwTJZCE1E1KZaVAM0adKkG94vLS29lViITEY/AvTj0WzklFZjQkQAxvb2xd7UQqQXVcLV3haPRHdqVd93hXtDbitGZoluGqxXQMPVYNvrp78aW/11rSB3XSF0NhMgIqI21aIRIBcXlxu+OnfujOnTp7c4iFWrViE4OBhyuRzR0dE4dOhQk20/++wzDB8+HG5ubnBzc0NMTEyD9oIgYOHChfDz84OdnR1iYmJw4cKFFsdF7dfwrp4Y2sUDggDsTyvGy/87gah3dmHBT6cAAHFDQuAga90aAHvp1WmwbY2sBsstrcaxzFKIbjD9paefAuMIEBFR22rRb4CvvvrK5AFs3LgR8+bNw5o1axAdHY3ly5cjNjYWqamp8Pb2btB+z549mDZtGoYMGQK5XI4lS5ZgzJgxOH36NAICdBvVLV26FCtWrMC6desQEhKCBQsWIDY2FmfOnIFc3vhGd9SxOMlt8e0/BiGrpAo/H8/F1mM5uFBQAXWdFg5SCWYM6XxL/Y/r7Y/tJxX49WQuXhlrvBpMP/01oLM7vJvYWFGvk34vIC6FJyJqUyKhuVva3ibR0dEYMGCAYWdprVaLoKAgPPvss3j11Vdv+rxGo4GbmxtWrlyJ6dOnQxAE+Pv744UXXsCLL74IACgrK4OPjw/Wrl2LqVOn3rRPpVIJFxcXlJWVwdnZ+dY+IFkEQRBwJk+JnWfyEdXZHcO6et5Sf1XqOkS+vQvVtRr8MmcYege6QKMVsONUHuK3n0NOaTUW3d8DcUNDbthPqqIcscv/hLPcBifeiL2lmIiIrF1Lfn+3eh8gU1Cr1UhOTsb8+fMN18RiMWJiYpCUlNSsPqqqqlBbWwt3d3cAQHp6OhQKBWJiYgxtXFxcEB0djaSkpEYTIJVKBZVKZfhZqbzx8mZqf0QiEXr6u7ToENMbsZfa4K47vPHriTz8lJKD8/nlWLXnIi4VVgIAvJxkuK8Z+wvpa4CUNXUoq66Fix1PhSciagtmXQVWVFQEjUYDHx/jYwh8fHygUCia1ccrr7wCf39/Q8Kjf64lfcbHxxvVMgUFBbX0o5AVGtdbtxrs833peGHTcVwqrISz3AZzY7pi5/Mj4OXU9OovPXupDTwddZsk8kgMIqK2066XwS9evBgbNmzAli1bbqm2Z/78+SgrKzO8srKyTBgldVR3dveGU30htYeDFK+MDcffr96FuTHd4GrfcOfnpvBMMCKitmfWKTBPT09IJBLk5+cbXc/Pz4ev741XzyxbtgyLFy/Grl27jE6g1z+Xn58PPz8/oz4jIiIa7Usmk0Emu/n/Wie6lp1UgnVPDkR6YSXu7e0HO2nrjrIIcrPHscxSFkITEbUhs44ASaVSREZGIjEx0XBNq9UiMTHRcMp8Y5YuXYq3334bCQkJiIqKMroXEhICX19foz6VSiUOHjx4wz6JWqN/JzdMjgxsdfIDXK0D4lJ4IqK2Y9YRIACYN28eZsyYgaioKAwcOBDLly9HZWUl4uLiAADTp09HQEAA4uPjAQBLlizBwoUL8d133yE4ONhQ1+Po6AhHR0eIRCLMnTsX77zzDrp27WpYBu/v748JEyaY62MSNcmwFL6Ex2EQEbUVsydAU6ZMQWFhIRYuXAiFQoGIiAgkJCQYipgzMzMhFl8dqFq9ejXUajUefPBBo34WLVqEN954AwDw8ssvo7KyEk8//TRKS0sxbNgwJCQkcA8gskiG88A4AkRE1GbMvg+QJeI+QNSWskqqMHzpbkglYpx7eyzEYtHNHyIiogZa8vu7Xa8CI+oI/FzkkIhFUGu0KChX3fyBFhAEAfsvFiG3lNNrRETXMvsUGJG1s5GIEeBqh8ySKmSWVMHXxXRTtRsOZ2H+5pMQiYDhXb3wUGQg7u7hA7lt64u2iYg6AiZARBYgyF2XAGWVVGFgiLtJ+hQEAev2Z9T/GfjzfCH+PF8IFztbTIjwx9MjwxDgameS9yIiam84BUZkAW7HqfDHs8twTlEOmY0YP88Zimfv6gI/FznKqmuxLukyXv3fCZO9FxFRe8MRICILEHQbToX//mAmAODe3n7oE+iKPoGumBvTDQmnFJj93VEcTC9BTa2G02FEZJU4AkRkAUx9HEZ5TS1+OZELAJg2sJPhukQswr29feHlJIO6ToujmVdM8n5ERO0NEyAiC2DqzRB/Pp6LKrUGYV4OGBDsZnRPJBJhSJgHAOBAWnGL+r1cXIlZ3yTj48QLJomTiMhcmAARWYAgN10xcn55DWpqNbfc34ZDugN9pw3sBJGo4b5Cg0N1CdD+ZiZAgiBg/YHLGLv8L+w4pcCKPy5Ao+UWYkTUfjEBIrIA7g5S2EslEAQg5xb37DmZXYaTOWWQSsSY1D+w0TaD60eAjmeXokpdd8P+8sqqMf3LQ1iw9RSq65OzWo2AfGXNLcVJRGROTICILIBIJLpmGuzW6oC+P6wrfo7t5Qt3B2mjbTq52yPA1Q61GgFHMpquA/r5eC5i//Mn/rpQBJmNGAvv64HOHqZfsUZE1NaYABFZiEATnAlWqarDzyn1xc8DgppsJxKJMOgm02DHs0rx7++PQVlTh76BLvj138PxxLAQQ6LGBIiI2jMmQEQWwjACdKX1U2DbTuSiQlWHYA97Q4LTFH0hdNKlxhOgbw9eBgDE9vTB/2YNQRdvR+M4mQARUTvGBIjIQgS56wqhM4tbn1h8X1/8PGVAp5seqqqvAzqZXQplTa3RPWVNLX45ngcAeHpEKGwkV/9TYeol+0RE5sAEiMhCdGpiM8SEUwrEfXUIK/+4gAv55RCEhquvyqpr8cORLKRklcJGLMKDkY0XP1/L39UOnT3soRWAw+klRvd+SslFda0GXb0d0b+T8TJ6ToERUUfAnaCJLMT1Iyu1Gi0W7ziHL/alAwB2pxZi2e/nEerpgDE9fREd6o5T2WXYe74Qx7JKDcvSY3vqNjpsjiFhHrhcXIWktGKMvsMHgG7Ju34X6amNLKO/mgDxhHkiar+YABFZiMD6vYCUNXVIVZTjtS0nkXxZt0Lr4ahAFJar8PfFYlwqqsSavWlYszfN6Pku3o4Y1c0L/47p2uz3HBTqge8PZRkVQp/MKcOZPCWkNmJM6hfQ4Bn9uWVFFSpUqzWwk/IoDSJqf5gAEVkIe6kNPB1lKKpQYcKqv1Fdq4GT3AbLHuqL2J6+AHRHXOxJLUTCaQVOZJeih58zRnbzxohunoZVZC2h3xDxrEKJ0io1XO2lhjqie3r5wq2RZfQu9rZwlttAWVOHrCtV6ObjdAufmojIPJgAEVmQIHc73chKrQZ3+DljzWP90dnDwXDfSW6L+/v64/6+/iZ5P29nObp4O+JiQQUOXCrB8K6e+DklB4DxGWLX6+Rhj1M5SmQWMwEiovaJRdBEFiSyvuD44ahAbHlmiFHyc7voR4GS0orwy/FcVKo1CPV0QHSIe5PPsBCaiNo7jgARWZD5996BmUODWzWd1VpDwjyw/sBlJF0qRkpWKQBg6sCgRs8Q09PXAV2/Yo2IqL1gAkRkQSRiUZsmPwAQXT8CdD6/AgBgKxFhchNniOlxLyAiau84BUZk5dwdpAj3vVrHM6aHLzwcb7yMnlNgRNTeMQEiIgwJ8zT8+UbFz3pXj8OobnRjxrZyqbACa/amoUJ14xPtiYiuxwSIiDCimy4B6uxhbzgj7Eb8Xe0gEgHVtRoUVahvd3hNmr/5JBbvOIe5G45BqzVfIkZE7Q8TICLCyG5e+GhqBL6cOeCmZ4gBgNRGDH+X+rPLzDQNln2lCgfrj/DYdbYAq3ZfNEscRNQ+MQEiIohEIoyPCECYl2Ozn9Ef3mquQuifUnIBAJ6Ous0aP9x1HntSC8wSCxG1P0yAiKhV9EvhzTECJAgCNh/NBgC8PDYcj0R3giAAz21I4co0ImoWJkBE1CqdWrEUPre0Ghfyy2/5vU/nKpFWWAmZjRj39PLFovt7oG+QK8qqa/Gvb5JRU6u55fcgoo6NCRARtUonj5aNAKnrtJj0yX6M+3gfLhdX3tJ7bzmmO67j7h4+cJLbQmYjwepH+8PDQYrTuUr839ZTLIomohviRohE1Cot3Qwx8Ww+FMoaAMCmI9l4MbZ7q963TqM11P9MvOa0en9XO3w8rR8e++IgfkzOxk8pOfBzsUOAqx0C3OwQ4umAx6I7w8XetlXvS0QdC0eAiKhV9DVAecoaqOpuPuW0KTnb8Of/Hc2GppUjNH+nFaOoQgV3BylGdPMyujekiyfeGt8LMhsxajUCMkuqkHSpGD8mZ+P931Lx+b5LrXpPIup4OAJERK3i6SiFna0E1bUa5JbWIMSz6YNbC5Q1hhVa9lIJ8spqsO9iEUZel8A0x9b66a/7+/jBVtLwf8M9Nqgzpg4IQn65CjlXqpFTWoVdZwrw68k8nFPcev0REXUMHAEiolYRiUTNPhJj87EcaAUgsrMbHorUnTO26UhWi9+zUlWHhFMKAMCEa6a/rmcjESPA1Q4DQ9wxsV8gpg4MAgCkF91a7RERdRxMgIio1YKakQAJgmBIdh6KDMRDUbpk5Pcz+Sitatku0r+fUaC6VoNgD3tEBLk2+zn96NTl4spWT70RUcfCBIiIWk2/GWL2DRKgY1mlSCushNxWjHF9/NDT3xnhvk5Q12nx8/HcFr3flmO69hP6BUAkuvmO1Xr+LnaQ1tcF5VypbtF7ElHHxASIiFqtOVNgm47oip/v7eUHJ7ktRCIRHq4fBdLfa46C8hrsu1AIwHj1V3OIxSKEeOhGgS4VVbToWSLqmJgAEVGr3SwBqlZrsK1+lOfBqEDD9Qn9AmArEeFkThnO5imb9V6/HM+DVgD6d3JFZ4+mC66bEuqle4Z1QEQEMAEioltwswTot9MKlKvqEOhmh0EhV0+Zd3eQYnS4D4DmjQJVqOrw5b50AC0f/dHT1wFdKmQCRERMgIjoFgTW7wVUXlOHsqraBvc3JeuKnx+MDGxwyvzDA3QjQltTcqCu097wfeK3n0VOaTU6udtjcmTgDds2RZ8AcQSIiAALSIBWrVqF4OBgyOVyREdH49ChQ022PX36NCZPnozg4GCIRCIsX768QZs33ngDIpHI6BUeHn4bPwGR9bKTSuDlJAPQcBQo+0oV9qcVAwAm92+YtIzo6gVvJxlKKtX441x+k++x/2IRvj2YCQBYPLk37KWt276MU2BEdC2zJkAbN27EvHnzsGjRIhw9ehR9+/ZFbGwsCgoKGm1fVVWF0NBQLF68GL6+vk3227NnT+Tl5Rle+/btu10fgcjqNTUN9r/kHAgCMDjUw7Bc/lo2EjEm9tdNZzU1DValrsMrm08AAB6N7oQhYZ6tjjPE0xEAkFNazcNSici8CdCHH36Ip556CnFxcejRowfWrFkDe3t7fPnll422HzBgAN5//31MnToVMpmsyX5tbGzg6+treHl6tv4/mkR0Y0FuuqXw1yZAFwvK8f0h3ajNQ1FNT1k9FKlbDbbnfCF+TM6GIBjv0bM0IRVZJdUIcLXD/HvvuKU43ext4WKnOwcs4xYPYyWi9s9sCZBarUZycjJiYmKuBiMWIyYmBklJSbfU94ULF+Dv74/Q0FA8+uijyMzMvNVwiagJ+hGgrCtVEAQB65MyMG7FPiiUNfB3kWNsr6ZHa7t4O+LuHj7QaAW8uOk4Hv38IDLqp6gOZ5RgXVIGACB+Um84ym7t5B6RSHS1DoiF0ERWz2wJUFFRETQaDXx8fIyu+/j4QKFQtLrf6OhorF27FgkJCVi9ejXS09MxfPhwlJc3fQaQSqWCUqk0ehFR8+int07nlOHJdUew4KfTUNVpMbyrJ7bMHnrTmp1PHu2PV8aGQ2Yjxv60YsQu/xMr/7iAl388AUEApkQFNTj0tLX0dUCXWAdEZPU63GGo99xzj+HPffr0QXR0NDp37owffvgBTz75ZKPPxMfH480332yrEIk6FP0I0PHsMgCA1EaMV8eGY+aQ4AYrvxpjKxFj1qgw3NvbF69vOYV9F4uw7PfzAAAfZxleG3drU1/XCm1nS+HzlTWoVNUh1MvR3KEQdThmGwHy9PSERCJBfr7x6o/8/PwbFji3lKurK7p164aLFy822Wb+/PkoKyszvLKyWn5II5G1unZTwnBfJ/w8ZyieGBbSrOTn+n7WPzkQHz7cF272thCLdFNf+rodU9AXQqe3g92giytUuPejv3Dvir9wpbJlZ6YR0c2ZbQRIKpUiMjISiYmJmDBhAgBAq9UiMTERc+bMMdn7VFRUIC0tDY8//niTbWQy2Q2Lqomoab4ucrw4phs0WuCfI0Mht5W0ui+RSIRJ/QNxdw8flFSqW7Xj8420p72A3t1+FsX1iU96cSXcHKRmjoioYzHrFNi8efMwY8YMREVFYeDAgVi+fDkqKysRFxcHAJg+fToCAgIQHx8PQFc4febMGcOfc3JykJKSAkdHR3Tp0gUA8OKLL+L+++9H586dkZubi0WLFkEikWDatGnm+ZBEVmDOXV1N2p+T3BZOctON/OgFe+qm665U1eJKpdpik4r9F4uw+WiO4ecCZY0ZoyHqmMyaAE2ZMgWFhYVYuHAhFAoFIiIikJCQYCiMzszMhFh8dZYuNzcX/fr1M/y8bNkyLFu2DCNHjsSePXsAANnZ2Zg2bRqKi4vh5eWFYcOG4cCBA/DyMk0RJRG1X/ZSG/i5yJFXVoNLRZWItMAEqKZWg9e3ngIAiESAIAD5SpWZoyLqeMxeBD1nzpwmp7z0SY1ecHBwg31CrrdhwwZThUZEHVCIpwPyymqQXlSJyM5u5g6ngU92X0R6USW8nWQY1tUTm4/mIJ8jQEQmZ/ajMIiI2tLVIzEsrxD6YkE5Vu9NAwC88UBPdPHWFW1zBIjI9JgAEZFVuboSzLIKobVaAa9tPoVajYC7wr1xTy9f+DjJAQAF5RwBIjI1JkBEZFUsdS+gH5OzcSijBHa2Erz5QE+IRCL4OOsSIE6BEZkeEyAisir6pfAZxZXQam9cU9hWSirVeG/HWQDA83d3Neyu7eOs256DU2BEpscEiIisSqCbHWzEItTUapFnISMrS3acQ2lVLcJ9nRA3NMRw3bt+BKisupYn2BOZGBMgIrIqNhIxOnnoRlgs4VDU5MtXsPGIbvf5dyb0gq3k6n+WneU2kNvqfi7gKBCRSTEBIiKrE9rCIzHKa2qRfaUK6UWVuJBfjjO5SpzKKYPmFqfQ6jRaLKjf8+fByEBEBbsb3TeqA2IhNJFJmX0fICKithbq5QCcbd6p8EcySjD10wOoayTZ6envjO+eGtTq88q+OXAZZ/KUcJbb4NV7whtt4+Mkx+XiKhZCE5kYR4CIyOo090wwQRCweMc51GkF2EpEcJTZwM3eFl5OMshtxTidq8QTaw+jSl3X4hgKymvwQf2p9y+NDYenY+PnEXqzEJrotuAIEBFZneYmQH9eKMKRy1cgsxHjr5fvNBQlA8DZPCWm/DcJyZev4J/rk/H5jCjIbJp/EGz89nMoV9WhT6ALHhnYqcl2+ikwngdGZFocASIiq6PfCyirpAqqusZXVwmCgA9/TwUAPD6os1HyAwB3+Dnjq7iBsLOV4K8LRZi7IQV1Gq1Rm4LyGnx3MBNr/07HrjP5OKdQokJVhwOXirHlWA5EIuDt8b0gEYuajPXqUngmQESmxBEgIrI6Xk4yOEglqFRrkFVShS7eTg3aJJ4twPHsMtjZSvCvUWGN9hPZ2Q2fTo/Ek2uPYMcpBeZvPomXYrsj4bQC207k4XBGCRo7vlCf8Ewb2Al9g1xvGOvVzRDNOwVWXlOLbw5kYnL/gAbJIFF7xASIiKyOSCRCiJcDTuUocamwskECpNUK+HCnrj5nxpDgJutzAGB4Vy+smNYPz3ybjE3J2diUnG10PyLIFb7OcmSXViH7SjVKq2qh0QrwdpLh5djuN43V28kyVoF9tOsCPt+Xjj2pBdjw9CCIRE2PWhG1B0yAiMgqhXo64lSOEmmN7AX022kFzuQp4SizwT9HhN60r7G9fLH0wb54cdNxALqk574+frintx8CXO2M2pbX1CKntBo+TnK42ktv2rdhCqzMfAmQRivg5+O5AICD6SXYnVqAu8J9zBYPkSkwASIiq9TNR7cX0IrEC5DZiDFjSDAkYhE0WgH/2aUb/XliaDDcHG6epAC6fXwiglxgJ7VpkPRcy0lui3Df5i+b1083Vao1qFDVwVHW9v/ZPnipGAXlV6fgluxIxchu3jesXSKydCyCJiKr9PjgYESHuKO6VoO3tp3BQ2v240J+ObadyMX5/Ao4y23w5PCbj/5cq4u30w2Tn9ZwlNkYkh5zFUL/lKIb/bm3ty9c7GyRml+O/x3NvslTRJaNCRARWSUXO1t8/9QgvDuxFxxlNjiaWYpxK/bhrV/OAACeGh7a6g0OTc3bjCvBVHUabD+VBwB4fFAwZt+pKwj/z87zPJ+M2jUmQERktcRiER6N7oyd80bgrnBvqDVaFFeq4WZvi7hhITfvoI34OOn3Amr7lWC7zxWivKYOvs5yRIe4Y/rgYAS42iGvrAZr92e0eTxEpsIEiIisnp+LHb6YEYWPpkYgIsgV707sbZZam6aYcy+gn4/nAAAeiPCHWCyC3FaCeXd3AwB8svsiSqvUbR4TkSkwASIigm5p/PiIAGydPRT39vYzdzhGzLUXUHlNLXadLQAAPNDX33B9Qr8AhPs6QVlTh1W7L7ZpTESmwgSIiMjCmetE+N9O50Ndp0WYlwN6+jsbrkvEIsPhrev2X0b2lao2jYvIFJgAERFZOHOdB/ZTim76a3xEQIOND0d288KQMA+oNVp8WH+oK1F7wgSIiMjC+dzkRPhvDlzGk2sPo6y61mTvWViuwt8XiwAYT3/piURXR4G2pOTgbJ7SZO9N1BaYABERWbirNUA1EK47XEwQBPxn53kknivApiNZJnvPbSdyoRWAvkGuCK4/PPZ6fQJdMa6PHwQBWJpwzmTvTdQWmAAREVk4LyfdCJCqTgtldZ3RvYziKhRX6lZibT6aY7L31G9+OCGi4ejPtV4c0x02YhF2pxYiKa3YZO9PdLsxASIisnByWwlc7XWbMl5fCJ18+Yrhz2fylEhVlN/y+10urkRKVinEImBcnxuviAvxdMC0gZ0AAIsTzjUYoSKyVEyAiIjaAf1miNfvBXRtAgQAW47d+ijQj/Un2g/t4mk4jf5Gnh3dBXa2EhzPKsVvpxW3/P5EbYEJEBFRO+DdRCH00foEaFK/AAC6lVtabetHYa5UqvHV3xkAgCkDgpoXm5McTw3X7Zy99LdU1Gm0rX5/orbCBIiIqB24thBar6y6FucLdFNeL8Z2h5PcBnllNTiQ3vpanDV701ChqsMdfs64t1fzN4R8akQo3B2kuFRYiU3JPCiVLB8TICKidkC/FP7avYBSskohCECwhz38Xe1wX329ztYmpsGUNbXYk1oATRMjRPnKq+d7vRTbDWKxqNF2jXGS22LOnV0A6A5KrVbzoFSybEyAiIjagcaOw0jOKAEA9O/sBgCYEKGbBttxUtHgpPZqtQYPr0nCzK8O47XNJxstVv74jwtQ1WkR2dkNd3b3bnGMjw7qhEA3OxSUq/DV/vQWP0/UlpgAERG1A/pi5GtXgSVn6up/IusToAHB7ghwtUO5qg67zuYb2gmCgNe3nsS5+hViG49kYc3eS0b9ZxZXYcMh3T5CL8V2b7Dzc3PIbCT4911dAQC/HM9r8fNEbYkJEBFRO2DYDbpMlwDVabRIySwFcDUBEotFmFhfDL3lmj2Bvj+Uhc1HcyARizBtoK6weUnCOfx64mqSsnzXedRpBQzv6olBoR6tjnNoV08AwPn8ck6DkUVjAkRE1A4YzgMrV0GrFZCaX45KtQZOMht09XYytJtQnwDtPV+I4goVTmSX4o2fTwPQjezET+qDmUOCAQDzfkjB0cwrOJ9fji315369FNv9luL0d5HD01EGjVbAmbyyW+qL6HZiAkRE1A7od4Ou0wooqVIblr9HdHKF5Jpi5S7ejugT6II6rYD1By5j1jdHodZoMaaHD/45IhQAsOC+Hoi5wxuqOi2e/voIFv50CoIAjO3piz6BrrcUp0gkQkSQCwAgJcv8CdDl4kocvHTzVXHqOi1PtbcyTICIiNoBW4kYno5SALrVWvoNEPXTX9fSF0Mv33UBOaXVCPawx7KH+xrqeiRiET6a2g89/Z1RVKHGgUslEImAF8Z0M0ms+iTqRHapSfprLXWdFlM/PYApnx7An+cLm2wnCAKe+voIhi3ZbfaYqe0wASIiaif0hdAFShWO3CABeiDC3zAqJLMR45NHI+EstzVq4yCzwRczBsC3fmptYr8AdPVxatBXa/QNcgUAHM8qNUl/rfXbaQXy6mum3t52pskNGnecUmBvfYJ0KL2kzeIj82ICRETUTugLoU9klyH7SjXEIiCiPtm4lqejDGN7+QIA3p3YGz38nRvtz9dFju+eisbsO8OwYFwPk8XZN1A3BZZRXIXSKrXJ+m2p9UmXDX++UFCB7w5lNmhTrdbg3V/PGn6+VFTZJrGR+TEBIiJqJ/SF0DtO6VZvdfd1htN1Izt6HzzUF3+9fCcejAy8YZ+hXo54KTYcbg5Sk8Xpai9FZw97ALpkzRzOKZQ4lFECiViEZ+/SbdD44c7zDRKy//6ZhpzSasPPlwor2jROMh8mQERE7YQ+AdLv5xPZ2bXJtnJbCYLc7dsirEb1ra8DMtc02Nf1oz+xPX3w3Oiu6O7jhNKqWnyUeMHQJqe0Gmv2pgGA4SyzdI4AWQ2zJ0CrVq1CcHAw5HI5oqOjcejQoSbbnj59GpMnT0ZwcDBEIhGWL19+y30SEbUX+gRIr7H6H0vRp34a7LgZRoCUNbWG40AeG9QZNhIxFtynm+Jbn3QZFwt0ozzvbT+LmlotokPcMedO3QaO+UoVKlR1bR4ztT2zJkAbN27EvHnzsGjRIhw9ehR9+/ZFbGwsCgoKGm1fVVWF0NBQLF68GL6+vibpk4iovdDXAOlFdnI3UyQ3p69NOp5d2uixG7fT5uRsVKk16OrtiMH1mzoO6+qJmDt8UKcV8O6vZ3DgUjF+PZEHsQhYdH9PuNjbwqN+GjCDo0BWwawJ0IcffoinnnoKcXFx6NGjB9asWQN7e3t8+eWXjbYfMGAA3n//fUydOhUymazRNi3tk4iovbh2BMjTUYYgdzszRnNjPf1dIBGLUFiuguKaA1xbo6y6FuU1tc1qKwi6/Y8A4PHBnY2O9Hh93B2wlYiwO7UQc747BgB4JLqToUg81MsBAJDGOiCrYLYESK1WIzk5GTExMVeDEYsRExODpKSkNu1TpVJBqVQavYiILI33NSNAkZ1dW3VeV1uxk0rQrX5Z/a3UASlrajH6g724d8VfqFLffGpqf1ox0gor4SCVGI4F0QvxdMCMwcEAgKIKFVzsbPHC3d2N7gOsA7IWZkuAioqKoNFo4OPjY3Tdx8cHCoWiTfuMj4+Hi4uL4RUUFNSq9yciup08HGSG/X0suf5HT78j9K3UAe08nY+iChWySqqxbv/lm7b/OikDADCpf2CjK+SeHd0V7vVTXfPu7ma0+i3UyxEAcKmQCZA1MHsRtCWYP38+ysrKDK+srCxzh0RE1IBELEKAq27aa2BI6w8sbSumWAm27USu4c9r9qZBeYOpsNzSauw8kw9AN/3VGBc7W6yLG4j3JvbGY4OM23AEyLrYmOuNPT09IZFIkJ+fb3Q9Pz+/yQLn29WnTCZrsqaIiMiS/GdKX1wsqGh0A0RLoz8S42R2GbRaAWJxy6bsyqpq8deFIgC6AvB8pQpf/JWO5+9u/MiO7w5mQisA0SHuhum3xvQOdEHv+lVq1wqrrwG6VFgBQRAseoqRbp3ZRoCkUikiIyORmJhouKbVapGYmIjBgwdbTJ9ERJYksrM7pgzoZO4wmqWbjyPktmKUq+patcPyb2cUqNMKCPd1wqL7ewIAvtiXjiuVDXeXziiqNBQ/T6+v82mpIHd7iEVApVqDwnJVq/qg9sOsU2Dz5s3DZ599hnXr1uHs2bOYNWsWKisrERcXBwCYPn065s+fb2ivVquRkpKClJQUqNVq5OTkICUlBRcvXmx2n0RE1DZsJGL08q+vA2rFNNi2E7odr+/r44exPX3Rw88ZFao6rPkzzajdlUo14tYeRll1LfoEumBMT5/Gurspmc3VzSPTWAfU4Zk1AZoyZQqWLVuGhQsXIiIiAikpKUhISDAUMWdmZiIvL8/QPjc3F/369UO/fv2Ql5eHZcuWoV+/fvjHP/7R7D6JiKjt6A9GbeyU9eTLV1BQ3vgS+SuVavx9UTf9Na6PP8RiEV6M1U19rdufYXhOVafBP9cnI72oEgGudvh8RhRsJa3/1cY6IOththogvTlz5mDOnDmN3tuzZ4/Rz8HBwc3aUOtGfRIRUdvRJ0Ap16wEEwQBSxJSsWZvGnyd5dg5b0SDFVsJpxXQaAX09Hc2JCV3dvdGv06uOJZZik92p2HR/T3w8o8ncCijBE4yG3wVNwDeTsa7ZbdUqKcj9qQW8kwwK8BVYEREdNvoT4Y/m6uEuk4LrVbAgp9OGc7gUihr8OHO8w2e+7V++mtcHz/DNZFIhJfG6Pbt+e5gJl7bcgo/peTCRizCmscjb1j43FwhXhwBshZMgIiI6Lbp5G4PV3tbqDVanMotw4ubjuObA5kQiYBpA3XF3Ov2Z+BUztURouIKFfan6aa/7uvtb9TfkC6eGBzqAbVGi+8PZQIA3pvUG0O7eJok3rD60abWFG1T+8IEiIiIbhuRSGRYDj/rm2RsPpYDiViE5VMiED+pNx7o6w+tALy+5SQ0Wl2Jw45TCmgF3YGqnTwanmivrwUCgDl3dsHDUabbvFY/ApRZUoVajdZk/ZLlYQJERES3VUT9NFi+UgWpRIzVj/bH+AjdMRX/d98dcJLb4Hh2Gb47qFvGbpj+6u3XaH+Rnd2x6P4eeCm2O14Y0/ieQK3l6yyHna0EGq2AzJIqk/ZNloUJEBER3VZRwbpT6+1sJfhy5gCM6Xl1Y1pvJzlejtXV9SxNSMXp3DIcTC8GANzbRAIEAHFDQzD7zi4m36xQJBJdXQlm4UvhtVoBCacURtOH1HxmXwVGREQd2/Cunvjgob7oHejSaKHyI9Gd8WNyNo5nl2H6F4egFYCIIFfDnjxtLdTLAWfylLhUVAHAMrdQySmtxss/HsffF4vh5STDoddGc+fqFuIIEBER3VYikQiTIwObXKUlEYvw7sTeEIuA4vpdnu/r0/Toz+0WasF7AQmCgB8OZyH2P3/i74u6kbLCchVySqvNHFn7wwSIiIjMrleAC2YMCTb8fKPpr9tNfyq8pe0GXaCswZPrjuDl/51AhaoO/Tu5olP9KNmZXKWZo2t/OAVGREQW4YUx3ZFWWImu3o7wrz/13hwsbTfo07ll2Hg4C1uO5qBcVQepRIwXxnTDP4aH4uUfTyCzpAqnc5VGtVV0c0yAiIjIIjjKbPD1EwPNHYZhKXxhuQrlNbUNdqluC+U1tfj5eC42Hs7CiWt20e4d4IIPHu5rmE7s4e+M/x0FzuRxBKilmAARERFdw1luC09HGYoqVEgvqjTsY9RWki9fQdxXh6CsqQMA2EpEGNPTF9MGdMKQMA+IxVeLnXv6OwPgFFhrMAEiIiK6TqiXA4oqVLhU2LYJ0KXCCvxj3WEoa+oQ4umARwZ2wqT+AfBwlDXa/g4/XQKUU1qN0io1XO2lJo9pd2oBjl6+gmfv6gqpTccpHWYCREREdJ0wLwccSi9p9ZEYRzJKsCThHAaHemDGkOAmE5hrFZarMOOrQ7hSVYu+gS74/ulBsJfe+Ne0i50tgtztkFVSjTO5Sgwx0ZEgegXKGsz+9iiq1Br4u9oZji/pCDpOKkdERGQi+kLo1pwKn15UiX98fQSHM65gxR8XMXTJH3jj59PIvtL0ztJV6jo8ue4wskqq0cndHl/MHHDT5EevR/0o0O2oA1r2eyqq1BoAwNdJlyEIgkn6rbOAY0aYABEREV0n1FO3FL6lK8FKq9R4cu1hlFbVoqe/M3oHuKCmVou1+zMw8v09eH5jCv44l4+yqlrDM3UaLeZ8dwwnssvgZm+LdU8MhGczRoz0evrrjho5beI6oFM5ZdiUnA1AV4d0Nk+J5MtXbrnf5MtXMPrDvTiUXnLLfd0KToERERFdR78SLL2oEoIgNGuXZXWdFrO+OYpLRZUIcLXD2riB8HSUYn9aMVbvScO+i0XYciwHW47lAAC6+TgiKtgdpVVq/HGuADIbMb6YOcAw+tRc+hGg07mmOxJDEAS88+sZCALwQF9/yGzE2JScja+TLhuONmmNClUdnt+YgsySKmw4lImBIa3v61YxASIiIrpOJ3d7SMQiVKk1yFeq4Osiv2F7QRCwYOspJF0qhoNUgi9mRsHLSTeKM7SLJ4Z28cTJ7DJ8e/AyDmWU4FJhJc7nV+B8vm6KTSwCPp7WD/07ubU41p4BugQorbASNbUayG0lLe7jer+fyceBSyWQ2Yjx8tjuuFJZi03J2dhxKg+F5T0Mn62l3vrlNDJLqhDgaoc3xve85ThvBRMgIiKi69hKxOjkbo/0okqkFVbcNAH69M9L2HgkC2IRsPKR/gj3dW7QpnegCxYH9gEAFFWokHz5Co5klOBUjhIPRQW2eiNDX2c53OxtcaWqFqmKcvQNcm1VP3rqOi3it58FAPxjeAgC3ewR6KY7ny0lqxQbD2dizl1dW9xvwqk8/HAkGyIR8OHDfeFshv2VrsUaICIiokaE++o2G3xp0/Ema1/qNFp89uclLE44BwBYcF8P3BnufdO+PR1liO3pi9fH9cD3Tw/CpP6BrY5TJBIZ6oBMUQj9dVIGMoqr4OUkw6xRXQzXpw/uDAD49mBmo0XMO8/kY/Z3Rxv9u8pX1uDVzScBAP8aGYboUI9bjvNWMQEiIiJqxEux3RHi6YDcshpM+W8S/rs3DVrt1VVQyZev4P6Vf+Pd7WchCMDjgzpj5jXnmbWlHv6mqQMqqVTjo8QLAIAXx3SDo+zqRNG9vf3g7iBFXlkNdp0tMHpux8k8/OubZPx6Ig8PrtmPN34+jQqVbiNHrVbAi5uOGwrDn4/pdksxmgoTICIiokaEejnil2eH4YG+/qjTCojfcQ7/+PoI0osqMX/zCUxevR9n85RwsbPFexN7480HejarWPp2MNWO0B/tOo/ymjrc4eeMByODjO7JbSV4OEp3bf2BDMP1xLP5ePb7Y9BoBXTzcYQgAGv3Z2DMh3vxx7l8rEvKwF8XiiCzEeOjqREWs5kia4CIiIia4CizwUdTIzAo1ANv/HIaf5wrwB/nro5+PBgZiPn3hDdro8PbSZ8Anc0rh0YrQCJueSKmrtMalr2/dm94o308Gt0J//0zDX9fLMbFggrkllZj1jdHUacVcH9ffyyfEoH9aUV4bctJZJVU44m1R6Dv5vVxd6CLt1PrP6SJWUYaRkREZKFEIhEeie6Erc8MRWj9EvXuPk744Z+DseyhvmZPfgAgxNMRclsxqms1yChu3e7VJ7JLUaXWwM3eFkPDGt9ROsjdHqPra5wW/nQKT68/ArVGi7E9ffHhw30hEYswvKsXfps7Ak+PCIVYBGgFYFR3Lzw+qHOrP9/twBEgIiKiZujh74xt/x6G41lliAp2g63EcsYQJGIRwn2dkZJVitO5SoR5Oba4j/1pxQCAwdcduHq9xwcHY9fZAkP7u8K9sWJaP6O/D3upDV679w480Ncfe88X4rHozmabHmyK5Xx7REREFs5eaoPBYR4Wlfzo9bjFOqD9aUUAgCFNjP7oDe/iadiscXhXT3zyaP8m63p6Bbhg9p1d4GJv3iXvjeEIEBERUQfQ8xZWgtXUanD0cikAYEjYjZeoi8UirHykH/ZdKML0wcEm2XjRHJgAERERdQCGQ1Fzlc0+vkMv+fIVqDVa+DrLm3UUR09/F8PeQ+2V5Y3hERERUYuF+zpDLAKKK9UoKFe16Nm/L+qnvzwsrlbndmECRERE1AHYSSUIrS9+bmkd0LUF0NaCCRAREVEHcX0dUIWqDl/uS8ddy/Zg8ur9UNVpGjyjrKnFiexSAMCQLjcugO5IWANERETUQfTwc8ZPKblIulSMclUdvjuYifIa3ZEUKKrET8dy8fAA4x2eD6eXQCsAwR72CHC1M0PU5sERICIiog5CX5j898Vi/HfvJZTX1CHU0wH39NKdNL/mT+PzzIBrp7+sZ/QH4AgQERFRh9E7wAVOMhuUq+owMMQdTw0Pxehwb1TVavD3xSJcKqzE72fyMbY+IQKMC6CtCRMgIiKiDsLF3hY/zRmKWo2A7r5Xz91ylNng8cGdsWp3GtbsTUNsTx+IRCIUV6hwTlEOwLoKoAFOgREREXUooV6ORsmP3swhIZDaiJGSVYqD6SUAgAOXdP833NcJnhZwpllbYgJERERkBbycZHgoMhAAsGZvGoCrx19Y2+gPwASIiIjIauhPaN+TWoizeUok1RdA3+z8r46ICRAREZGV6OzhgHt6+wEA3vrlDC4VVUIsAgaGuJs5srbHBIiIiMiKzBoZBgBIuqQb/ekd6AoXO8s7rf12s4gEaNWqVQgODoZcLkd0dDQOHTp0w/abNm1CeHg45HI5evfuje3btxvdnzlzJkQikdFr7Nixt/MjEBERtQu9Alww7Jodn61t+bue2ROgjRs3Yt68eVi0aBGOHj2Kvn37IjY2FgUFBY22379/P6ZNm4Ynn3wSx44dw4QJEzBhwgScOnXKqN3YsWORl5dneH3//fdt8XGIiIgs3r/qR4EA602ARIIgCDdvdvtER0djwIABWLlyJQBAq9UiKCgIzz77LF599dUG7adMmYLKykps27bNcG3QoEGIiIjAmjVrAOhGgEpLS7F169ZWxaRUKuHi4oKysjI4Ozu3qg8iIiJLJQgCnt+YgnylCmufGACZjcTcIZlES35/m3UESK1WIzk5GTExMYZrYrEYMTExSEpKavSZpKQko/YAEBsb26D9nj174O3tje7du2PWrFkoLi5uMg6VSgWlUmn0IiIi6qhEIhGWT+2H758e1GGSn5YyawJUVFQEjUYDHx8fo+s+Pj5QKBSNPqNQKG7afuzYsfj666+RmJiIJUuWYO/evbjnnnug0TQ8BRcA4uPj4eLiYngFBQU12o6IiIg6hg55FMbUqVMNf+7duzf69OmDsLAw7NmzB6NHj27Qfv78+Zg3b57hZ6VSySSIiIioAzPrCJCnpyckEgny8/ONrufn58PX17fRZ3x9fVvUHgBCQ0Ph6emJixcvNnpfJpPB2dnZ6EVEREQdl1kTIKlUisjISCQmJhquabVaJCYmYvDgwY0+M3jwYKP2ALBz584m2wNAdnY2iouL4efnZ5rAiYiIqF0z+zL4efPm4bPPPsO6detw9uxZzJo1C5WVlYiLiwMATJ8+HfPnzze0f+6555CQkIAPPvgA586dwxtvvIEjR45gzpw5AICKigq89NJLOHDgADIyMpCYmIjx48ejS5cuiI2NNctnJCIiIsti9hqgKVOmoLCwEAsXLoRCoUBERAQSEhIMhc6ZmZkQi6/maUOGDMF3332H//u//8Nrr72Grl27YuvWrejVqxcAQCKR4MSJE1i3bh1KS0vh7++PMWPG4O2334ZMZl0n3RIREVHjzL4PkCXiPkBERETtT7vZB4iIiIjIHJgAERERkdVhAkRERERWhwkQERERWR0mQERERGR1mAARERGR1WECRERERFbH7BshWiL91khKpdLMkRAREVFz6X9vN2eLQyZAjSgvLwcAnghPRETUDpWXl8PFxeWGbbgTdCO0Wi1yc3Ph5OQEkUhk0r6VSiWCgoKQlZXFXaYtBL8Ty8PvxDLxe7E8/E6MCYKA8vJy+Pv7Gx2j1RiOADVCLBYjMDDwtr6Hs7Mz/7FaGH4nloffiWXi92J5+J1cdbORHz0WQRMREZHVYQJEREREVocJUBuTyWRYtGgRZDKZuUOhevxOLA+/E8vE78Xy8DtpPRZBExERkdXhCBARERFZHSZAREREZHWYABEREZHVYQJEREREVocJUBtatWoVgoODIZfLER0djUOHDpk7JKsRHx+PAQMGwMnJCd7e3pgwYQJSU1ON2tTU1GD27Nnw8PCAo6MjJk+ejPz8fDNFbH0WL14MkUiEuXPnGq7xOzGPnJwcPPbYY/Dw8ICdnR169+6NI0eOGO4LgoCFCxfCz88PdnZ2iImJwYULF8wYccem0WiwYMEChISEwM7ODmFhYXj77beNzrvid9JyTIDayMaNGzFv3jwsWrQIR48eRd++fREbG4uCggJzh2YV9u7di9mzZ+PAgQPYuXMnamtrMWbMGFRWVhraPP/88/jll1+wadMm7N27F7m5uZg0aZIZo7Yehw8fxn//+1/06dPH6Dq/k7Z35coVDB06FLa2ttixYwfOnDmDDz74AG5uboY2S5cuxYoVK7BmzRocPHgQDg4OiI2NRU1NjRkj77iWLFmC1atXY+XKlTh79iyWLFmCpUuX4uOPPza04XfSCgK1iYEDBwqzZ882/KzRaAR/f38hPj7ejFFZr4KCAgGAsHfvXkEQBKG0tFSwtbUVNm3aZGhz9uxZAYCQlJRkrjCtQnl5udC1a1dh586dwsiRI4XnnntOEAR+J+byyiuvCMOGDWvyvlarFXx9fYX333/fcK20tFSQyWTC999/3xYhWp1x48YJTzzxhNG1SZMmCY8++qggCPxOWosjQG1ArVYjOTkZMTExhmtisRgxMTFISkoyY2TWq6ysDADg7u4OAEhOTkZtba3RdxQeHo5OnTrxO7rNZs+ejXHjxhn93QP8Tszl559/RlRUFB566CF4e3ujX79++Oyzzwz309PToVAojL4XFxcXREdH83u5TYYMGYLExEScP38eAHD8+HHs27cP99xzDwB+J63Fw1DbQFFRETQaDXx8fIyu+/j44Ny5c2aKynpptVrMnTsXQ4cORa9evQAACoUCUqkUrq6uRm19fHygUCjMEKV12LBhA44ePYrDhw83uMfvxDwuXbqE1atXY968eXjttddw+PBh/Pvf/4ZUKsWMGTMMf/eN/feM38vt8eqrr0KpVCI8PBwSiQQajQbvvvsuHn30UQDgd9JKTIDI6syePRunTp3Cvn37zB2KVcvKysJzzz2HnTt3Qi6XmzscqqfVahEVFYX33nsPANCvXz+cOnUKa9aswYwZM8wcnXX64Ycf8O233+K7775Dz549kZKSgrlz58Lf35/fyS3gFFgb8PT0hEQiabB6JT8/H76+vmaKyjrNmTMH27Ztw+7duxEYGGi47uvrC7VajdLSUqP2/I5un+TkZBQUFKB///6wsbGBjY0N9u7dixUrVsDGxgY+Pj78TszAz88PPXr0MLp2xx13IDMzEwAMf/f871nbeemll/Dqq69i6tSp6N27Nx5//HE8//zziI+PB8DvpLWYALUBqVSKyMhIJCYmGq5ptVokJiZi8ODBZozMegiCgDlz5mDLli34448/EBISYnQ/MjIStra2Rt9RamoqMjMz+R3dJqNHj8bJkyeRkpJieEVFReHRRx81/JnfSdsbOnRogy0izp8/j86dOwMAQkJC4Ovra/S9KJVKHDx4kN/LbVJVVQWx2PjXtUQigVarBcDvpNXMXYVtLTZs2CDIZDJh7dq1wpkzZ4Snn35acHV1FRQKhblDswqzZs0SXFxchD179gh5eXmGV1VVlaHNv/71L6FTp07CH3/8IRw5ckQYPHiwMHjwYDNGbX2uXQUmCPxOzOHQoUOCjY2N8O677woXLlwQvv32W8He3l745ptvDG0WL14suLq6Cj/99JNw4sQJYfz48UJISIhQXV1txsg7rhkzZggBAQHCtm3bhPT0dGHz5s2Cp6en8PLLLxva8DtpOSZAbejjjz8WOnXqJEilUmHgwIHCgQMHzB2S1QDQ6Ourr74ytKmurhaeeeYZwc3NTbC3txcmTpwo5OXlmS9oK3R9AsTvxDx++eUXoVevXoJMJhPCw8OFTz/91Oi+VqsVFixYIPj4+AgymUwYPXq0kJqaaqZoOz6lUik899xzQqdOnQS5XC6EhoYKr7/+uqBSqQxt+J20nEgQrtlKkoiIiMgKsAaIiIiIrA4TICIiIrI6TICIiIjI6jABIiIiIqvDBIiIiIisDhMgIiIisjpMgIiIiMjqMAEiIiIiq8MEiIjaheDgYCxfvtzcYWDBggV4+umnzR1GA0VFRfD29kZ2dra5QyFqF5gAEZGRmTNnYsKECYafR40ahblz57bZ+69duxaurq4Nrh8+fNjsiYdCocBHH32E119/vdnP5OXl4ZFHHkG3bt0gFoub/LvctGkTwsPDIZfL0bt3b2zfvt3oviAIWLhwIfz8/GBnZ4eYmBhcuHDBcN/T0xPTp0/HokWLWvXZiKwNEyAiahNqtfqWnvfy8oK9vb2Jommdzz//HEOGDDGcjN4cKpUKXl5e+L//+z/07du30Tb79+/HtGnT8OSTT+LYsWOYMGECJkyYgFOnThnaLF26FCtWrMCaNWtw8OBBODg4IDY2FjU1NYY2cXFx+Pbbb1FSUtL6D0lkLcx8FhkRWZgZM2YI48ePN/wZ1x0gm56eLgiCIJw8eVIYO3as4ODgIHh7ewuPPfaYUFhYaOhn5MiRwuzZs4XnnntO8PDwEEaNGiUIgiB88MEHQq9evQR7e3shMDBQmDVrllBeXi4IgiDs3r27wfstWrRIEARB6Ny5s/Cf//zH0P/ly5eFBx54QHBwcBCcnJyEhx56SFAoFIb7ixYtEvr27St8/fXXQufOnQVnZ2dhypQpglKpNLTZtGmT0KtXL0Eulwvu7u7C6NGjhYqKiib/bnr27CmsXLnS8HNBQYHg4+MjvPvuu4Zrf//9t2Brayvs2rWrwfPXH/aq9/DDDwvjxo0zuhYdHS3885//FARBd9Clr6+v8P777xvul5aWCjKZTPj++++NngsJCRE+//zzJj8DEelwBIiImvTRRx9h8ODBeOqpp5CXl4e8vDwEBQWhtLQUd911F/r164cjR44gISEB+fn5ePjhh42eX7duHaRSKf7++2+sWbMGACAWi7FixQqcPn0a69atwx9//IGXX34ZADBkyBAsX74czs7Ohvd78cUXG8Sl1Woxfvx4lJSUYO/evdi5cycuXbqEKVOmGLVLS0vD1q1bsW3bNmzbtg179+7F4sWLAeimpqZNm4YnnngCZ8+exZ49ezBp0iQITZwPXVJSgjNnziAqKspwzcvLC19++SXeeOMNHDlyBOXl5Xj88ccxZ84cjB49utl/z0lJSYiJiTG6Fhsbi6SkJABAeno6FAqFURsXFxdER0cb2ugNHDgQf/31V7Pfm8ha2Zg7ACKyXC4uLpBKpbC3t4evr6/h+sqVK9GvXz+89957hmtffvklgoKCcP78eXTr1g0A0LVrVyxdutSoz2trYIKDg/HOO+/gX//6Fz755BNIpVK4uLhAJBIZvd/1EhMTcfLkSaSnpyMoKAgA8PXXX6Nnz544fPgwBgwYAECXKK1duxZOTk4AgMcffxyJiYl49913kZeXh7q6OkyaNMkwpdW7d+8m3zMzMxOCIMDf39/o+r333ounnnoKjz76KKKiouDg4ID4+Pgm+2mMQqGAj4+P0TUfHx8oFArDff21ptro+fv749ixYy16fyJrxBEgImqx48ePY/fu3XB0dDS8wsPDAehGXfQiIyMbPLtr1y6MHj0aAQEBcHJywuOPP47i4mJUVVU1+/3Pnj2LoKAgQ/IDAD169ICrqyvOnj1ruBYcHGxIfgDAz88PBQUFAIC+ffti9OjR6N27Nx566CF89tlnuHLlSpPvWV1dDQCQy+UN7i1btgx1dXXYtGkTvv32W8hksmZ/FlOzs7Nr0d8lkbViAkRELVZRUYH7778fKSkpRq8LFy5gxIgRhnYODg5Gz2VkZOC+++5Dnz598L///Q/JyclYtWoVgFsvkm6Mra2t0c8ikQharRYAIJFIsHPnTuzYsQM9evTAxx9/jO7duyM9Pb3Rvjw9PQGg0SQpLS0Nubm50Gq1yMjIaHGcvr6+yM/PN7qWn59vGAXT/98btdErKSmBl5dXi2MgsjZMgIjohqRSKTQajdG1/v374/Tp0wgODkaXLl2MXtcnPddKTk6GVqvFBx98gEGDBqFbt27Izc296ftd74477kBWVhaysrIM186cOYPS0lL06NGj2Z9NJBJh6NChePPNN3Hs2DFIpVJs2bKl0bZhYWFwdnbGmTNnjK6r1Wo89thjmDJlCt5++2384x//MIwyNdfgwYORmJhodG3nzp0YPHgwACAkJAS+vr5GbZRKJQ4ePGhoo3fq1Cn069evRe9PZI2YABHRDQUHB+PgwYPIyMhAUVERtFotZs+ejZKSEkybNg2HDx9GWloafvvtN8TFxd0weenSpQtqa2vx8ccf49KlS1i/fr2hOPra96uoqEBiYiKKiooanc6JiYlB79698eijj+Lo0aM4dOgQpk+fjpEjRxoVKd/IwYMH8d577+HIkSPIzMzE5s2bUVhYiDvuuKPR9mKxGDExMdi3b5/R9ddffx1lZWVYsWIFXnnlFXTr1g1PPPGEURv9CFlFRQUKCwuRkpJilEg999xzSEhIwAcffIBz584ZiqrnzJkDQJeozZ07F++88w5+/vlnnDx5EtOnT4e/v7/Rnk1VVVVITk7GmDFjmvV3QGTVzL0MjYgsy7XL4AVBEFJTU4VBgwYJdnZ2Rsvgz58/L0ycOFFwdXUV7OzshPDwcGHu3LmCVqsVBKHpJd8ffvih4OfnJ9jZ2QmxsbHC119/LQAQrly5Ymjzr3/9S/Dw8DDJMvhr/ec//xE6d+4sCIIgnDlzRoiNjRW8vLwEmUwmdOvWTfj4449v+Hezfft2ISAgQNBoNIIg6Jbt29jYCH/99ZehTXp6uuDs7Cx88sknhmu4bmk/AEMcej/88IPQrVs3QSqVCj179hR+/fVXo/tarVZYsGCB4OPjI8hkMmH06NFCamqqUZvvvvtO6N69+w0/AxHpiAShiTWfRERkRBAEREdH4/nnn8e0adPMHU4DgwYNwr///W888sgj5g6FyOJxCoyIqJlEIhE+/fRT1NXVmTuUBoqKijBp0iSLTMyILBFHgIiIiMjqcASIiIiIrA4TICIiIrI6TICIiIjI6jABIiIiIqvDBIiIiIisDhMgIiIisjpMgIiIiMjqMAEiIiIiq8MEiIiIiKzO/wMcQhBJUY7EagAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            losses.append(running_loss / 100)\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Display training loss\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations (x100)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8c052d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 91.2 %\n"
     ]
    }
   ],
   "source": [
    "# Test the network\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646c2a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully exported to fashion_mnist_cnn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18157/2155208815.py:29: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "\n",
    "# Export the model to ONNX format\n",
    "onnx_file_path = \"fashion_mnist_cnn.onnx\"\n",
    "\n",
    "net.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 28, 28, requires_grad=True).to(device)\n",
    "\n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "\n",
    "dynamic_axes = {'input': {0: 'batch_size'},\n",
    "                'output': {0: 'batch_size'}}\n",
    "\n",
    "\n",
    "# Perform the export\n",
    "torch.onnx.export(\n",
    "    net,\n",
    "    dummy_input,\n",
    "    onnx_file_path,\n",
    "    export_params=True,\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    dynamic_axes=dynamic_axes\n",
    ")\n",
    "\n",
    "print(f\"Model successfully exported to {onnx_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd547d",
   "metadata": {},
   "source": [
    "### 2. Benchmarking du modele\n",
    "\n",
    "On mesure le temps moyen de prÃ©diction du modÃ¨le original et le modÃ¨le onnx CPU (pour une comparaison equitable) pour diffÃ©rentes tailles de batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fc56491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference time in milliseconds (lower is better):\n",
      "+-------------------+----------------+----------------+-----------------+------------------+\n",
      "| Model             |   Batch Size 1 |   Batch Size 8 |   Batch Size 32 |   Batch Size 128 |\n",
      "+===================+================+================+=================+==================+\n",
      "| PyTorch (ms)      |           0.29 |           1.24 |            4.54 |            17.12 |\n",
      "+-------------------+----------------+----------------+-----------------+------------------+\n",
      "| ONNX Runtime (ms) |           0.12 |           0.36 |            1.36 |             0.74 |\n",
      "+-------------------+----------------+----------------+-----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "import torch\n",
    "\n",
    "def benchmark_pytorch_model(model, input_shape, batch_sizes=[1, 8, 32, 128], num_runs=100):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    channels, height, width = input_shape\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        dummy_input = torch.randn(batch_size, channels, height, width).to(device)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy_input)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_runs):\n",
    "                _ = model(dummy_input)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time = (end_time - start_time) * 1000 / num_runs\n",
    "        results[batch_size] = avg_time\n",
    "    \n",
    "    return results\n",
    "\n",
    "def benchmark_onnx_model(onnx_path, input_shape, batch_sizes=[1, 8, 32, 128], num_runs=100):\n",
    "    results = {}\n",
    "    \n",
    "    session = onnxruntime.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    \n",
    "    channels, height, width = input_shape\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        dummy_input = np.random.randn(batch_size, channels, height, width).astype(np.float32)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            _ = session.run(None, {input_name: dummy_input})\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            _ = session.run(None, {input_name: dummy_input})\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time = (end_time - start_time) * 1000 / num_runs\n",
    "        results[batch_size] = avg_time\n",
    "    \n",
    "    return results\n",
    "    \n",
    "\n",
    "# Move model to CPU for fair comparison\n",
    "net = net.cpu()\n",
    "device = torch.device(\"cpu\")\n",
    "input_shape = (1, 28, 28)  # Fashion-MNIST images are 28x28x1\n",
    "\n",
    "batch_sizes = [1, 8, 32, 128]\n",
    "pytorch_results = benchmark_pytorch_model(net, input_shape, batch_sizes)\n",
    "onnx_results = benchmark_onnx_model(\"fashion_mnist_cnn.onnx\", input_shape, batch_sizes)\n",
    "\n",
    "# Prepare results table\n",
    "headers = [\"Model\"] + [f\"Batch Size {bs}\" for bs in batch_sizes]\n",
    "data = [\n",
    "    [\"PyTorch (ms)\"] + [f\"{pytorch_results[bs]:.2f}\" for bs in batch_sizes],\n",
    "    [\"ONNX Runtime (ms)\"] + [f\"{onnx_results[bs]:.2f}\" for bs in batch_sizes]\n",
    "]\n",
    "\n",
    "print(\"\\nInference time in milliseconds (lower is better):\")\n",
    "print(tabulate(data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe8b66",
   "metadata": {},
   "source": [
    "Comparaison de la test accuracy entre le modÃ¨le PyTorch et le modÃ¨le ONNX exportÃ©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7d43ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed comparison of first 5 samples:\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "True label: 9\n",
      "PyTorch prediction: 9 (confidence: 1.0000)\n",
      "ONNX prediction: 9 (confidence: 1.0000)\n",
      "Max difference in softmax outputs: 0.000000\n",
      "\n",
      "Sample 2:\n",
      "True label: 2\n",
      "PyTorch prediction: 2 (confidence: 1.0000)\n",
      "ONNX prediction: 2 (confidence: 1.0000)\n",
      "Max difference in softmax outputs: 0.000000\n",
      "\n",
      "Sample 3:\n",
      "True label: 1\n",
      "PyTorch prediction: 1 (confidence: 1.0000)\n",
      "ONNX prediction: 1 (confidence: 1.0000)\n",
      "Max difference in softmax outputs: 0.000000\n",
      "\n",
      "Sample 4:\n",
      "True label: 1\n",
      "PyTorch prediction: 1 (confidence: 1.0000)\n",
      "ONNX prediction: 1 (confidence: 1.0000)\n",
      "Max difference in softmax outputs: 0.000000\n",
      "\n",
      "Sample 5:\n",
      "True label: 6\n",
      "PyTorch prediction: 6 (confidence: 0.9483)\n",
      "ONNX prediction: 6 (confidence: 0.9483)\n",
      "Max difference in softmax outputs: 0.000000\n",
      "\n",
      "Overall Results:\n",
      "PyTorch Accuracy: 87.50%\n",
      "ONNX Runtime Accuracy: 87.50%\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compare_predictions(pytorch_model, onnx_path, test_loader, num_samples=5):\n",
    "    pytorch_model.eval()\n",
    "    session = onnxruntime.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    \n",
    "    correct_pytorch = 0\n",
    "    correct_onnx = 0\n",
    "    total = 0\n",
    "    \n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Calculate accuracy and compare predictions\n",
    "    with torch.no_grad():\n",
    "        # PyTorch predictions\n",
    "        pytorch_outputs = pytorch_model(images)\n",
    "        _, predicted_pytorch = torch.max(pytorch_outputs.data, 1)\n",
    "        correct_pytorch += (predicted_pytorch == labels).sum().item()\n",
    "        \n",
    "        # ONNX Runtime predictions\n",
    "        onnx_outputs = session.run(None, {input_name: images.numpy()})\n",
    "        predicted_onnx = np.argmax(onnx_outputs[0], axis=1)\n",
    "        correct_onnx += (predicted_onnx == labels.numpy()).sum()\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        \n",
    "        print(\"\\nDetailed comparison of first\", num_samples, \"samples:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        pytorch_softmax = F.softmax(pytorch_outputs, dim=1).numpy()\n",
    "        onnx_softmax = softmax(onnx_outputs[0], axis=1)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            print(f\"True label: {labels[i]}\")\n",
    "            print(f\"PyTorch prediction: {predicted_pytorch[i]} (confidence: {pytorch_softmax[i][predicted_pytorch[i]]:.4f})\")\n",
    "            print(f\"ONNX prediction: {predicted_onnx[i]} (confidence: {onnx_softmax[i][predicted_onnx[i]]:.4f})\")\n",
    "            print(f\"Max difference in softmax outputs: {np.max(np.abs(pytorch_softmax[i] - onnx_softmax[i])):.6f}\")\n",
    "    \n",
    "    print(\"\\nOverall Results:\")\n",
    "    print(f\"PyTorch Accuracy: {100 * correct_pytorch / total:.2f}%\")\n",
    "    print(f\"ONNX Runtime Accuracy: {100 * correct_onnx / total:.2f}%\")\n",
    "    \n",
    "    return pytorch_softmax, onnx_softmax\n",
    "\n",
    "net = net.cpu()\n",
    "net.eval()\n",
    "\n",
    "pytorch_softmax, onnx_softmax = compare_predictions(net, \"fashion_mnist_cnn.onnx\", test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32967dfc",
   "metadata": {},
   "source": [
    "_4. Mesurer la test accuracy de votre modÃ¨le avec ONNX runtime et comparer les softpredictions sur plusieurs exemples. y-t-il des diffÃ©rences ?_\n",
    "\n",
    "On remarque que les tests accuracies sont Ã©quivalentes entre les deux modÃ¨les.\n",
    "\n",
    "### 3. Quantification du modÃ¨le ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f89b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 1.61 MB\n",
      "Quantized model size: 0.41 MB\n",
      "Size reduction: 74.63%\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = \"fashion_mnist_cnn.onnx\"\n",
    "model_quant = \"fashion_mnist_cnn_quantized.onnx\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=model_fp32,\n",
    "    model_output=model_quant,\n",
    "    weight_type=QuantType.QUInt8\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "original_size = os.path.getsize(model_fp32) / (1024 * 1024)  # Size in MB\n",
    "quantized_size = os.path.getsize(model_quant) / (1024 * 1024)  # Size in MB\n",
    "\n",
    "print(f\"Original model size: {original_size:.2f} MB\")\n",
    "print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "print(f\"Size reduction: {(1 - quantized_size/original_size)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a5d89",
   "metadata": {},
   "source": [
    "### Benchmark du modÃ¨le quantifiÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fed04238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference time in milliseconds (lower is better):\n",
      "+----------------+----------------+----------------+-----------------+------------------+\n",
      "| Model          |   Batch Size 1 |   Batch Size 8 |   Batch Size 32 |   Batch Size 128 |\n",
      "+================+================+================+=================+==================+\n",
      "| PyTorch (ms)   |           0.3  |           1.04 |            4.03 |            16.52 |\n",
      "+----------------+----------------+----------------+-----------------+------------------+\n",
      "| ONNX FP32 (ms) |           0.09 |           0.44 |            1.56 |             6.19 |\n",
      "+----------------+----------------+----------------+-----------------+------------------+\n",
      "| ONNX INT8 (ms) |           0.15 |           0.6  |            2.06 |            12.31 |\n",
      "+----------------+----------------+----------------+-----------------+------------------+\n",
      "\n",
      "Comparing accuracy between original and quantized models:\n",
      "\n",
      "Detailed comparison of first 5 samples:\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "True label: 9\n",
      "PyTorch prediction: 9 (confidence: 1.0000)\n",
      "ONNX prediction: 9 (confidence: 1.0000)\n",
      "Max difference in softmax outputs: 0.000000\n",
      "\n",
      "Sample 2:\n",
      "True label: 2\n",
      "PyTorch prediction: 2 (confidence: 1.0000)\n",
      "ONNX prediction: 2 (confidence: 1.0000)\n",
      "Max difference in softmax outputs: 0.000000\n",
      "\n",
      "Sample 3:\n",
      "True label: 1\n",
      "PyTorch prediction: 1 (confidence: 1.0000)\n",
      "ONNX prediction: 1 (confidence: 1.0000)\n",
      "Max difference in softmax outputs: 0.000000\n",
      "\n",
      "Sample 4:\n",
      "True label: 1\n",
      "PyTorch prediction: 1 (confidence: 1.0000)\n",
      "ONNX prediction: 1 (confidence: 1.0000)\n",
      "Max difference in softmax outputs: 0.000000\n",
      "\n",
      "Sample 5:\n",
      "True label: 6\n",
      "PyTorch prediction: 6 (confidence: 0.9483)\n",
      "ONNX prediction: 6 (confidence: 0.9405)\n",
      "Max difference in softmax outputs: 0.007805\n",
      "\n",
      "Overall Results:\n",
      "PyTorch Accuracy: 87.50%\n",
      "ONNX Runtime Accuracy: 87.50%\n"
     ]
    }
   ],
   "source": [
    "quant_results = benchmark_onnx_model(model_quant, batch_sizes)\n",
    "\n",
    "headers = [\"Model\"] + [f\"Batch Size {bs}\" for bs in batch_sizes]\n",
    "data = [\n",
    "    [\"PyTorch (ms)\"] + [f\"{pytorch_results[bs]:.2f}\" for bs in batch_sizes],\n",
    "    [\"ONNX FP32 (ms)\"] + [f\"{onnx_results[bs]:.2f}\" for bs in batch_sizes],\n",
    "    [\"ONNX INT8 (ms)\"] + [f\"{quant_results[bs]:.2f}\" for bs in batch_sizes]\n",
    "]\n",
    "\n",
    "print(\"\\nInference time in milliseconds (lower is better):\")\n",
    "print(tabulate(data, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "print(\"\\nComparing accuracy between original and quantized models:\")\n",
    "_, _ = compare_predictions(net, model_quant, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b0e29",
   "metadata": {},
   "source": [
    "### Meme analyse mais avec un dataset plus large (CIFAR-10) et un modÃ¨le plus complexe (ResNet18)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d62ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 170M/170M [00:06<00:00, 25.2MB/s] \n",
      "/home/leon/miniconda3/envs/mlops/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/leon/miniconda3/envs/mlops/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/leon/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 44.7M/44.7M [00:00<00:00, 104MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture: ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "CIFAR-10 Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the last layer for CIFAR-10 (10 classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model architecture:\", model)\n",
    "print(\"\\nCIFAR-10 Classes:\", trainset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0e83f",
   "metadata": {},
   "source": [
    "#### Fine-tuning sur CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc7c72fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1, 100] loss: 2.497\n",
      "Epoch [1, 200] loss: 1.906\n",
      "Epoch [1, 300] loss: 1.749\n",
      "Epoch [1, 400] loss: 1.568\n",
      "Epoch [1, 500] loss: 1.389\n",
      "Epoch [1, 600] loss: 1.295\n",
      "Epoch [1, 700] loss: 1.153\n",
      "Epoch [2, 100] loss: 1.026\n",
      "Epoch [2, 200] loss: 0.969\n",
      "Epoch [2, 300] loss: 0.948\n",
      "Epoch [2, 400] loss: 0.922\n",
      "Epoch [2, 500] loss: 0.909\n",
      "Epoch [2, 600] loss: 0.862\n",
      "Epoch [2, 700] loss: 0.814\n",
      "Epoch [3, 100] loss: 0.757\n",
      "Epoch [3, 200] loss: 0.777\n",
      "Epoch [3, 300] loss: 0.773\n",
      "Epoch [3, 400] loss: 0.736\n",
      "Epoch [3, 500] loss: 0.716\n",
      "Epoch [3, 600] loss: 0.698\n",
      "Epoch [3, 700] loss: 0.691\n",
      "Epoch [4, 100] loss: 0.628\n",
      "Epoch [4, 200] loss: 0.629\n",
      "Epoch [4, 300] loss: 0.611\n",
      "Epoch [4, 400] loss: 0.599\n",
      "Epoch [4, 500] loss: 0.617\n",
      "Epoch [4, 600] loss: 0.618\n",
      "Epoch [4, 700] loss: 0.591\n",
      "Epoch [5, 100] loss: 0.466\n",
      "Epoch [5, 200] loss: 0.496\n",
      "Epoch [5, 300] loss: 0.487\n",
      "Epoch [5, 400] loss: 0.520\n",
      "Epoch [5, 500] loss: 0.535\n",
      "Epoch [5, 600] loss: 0.535\n",
      "Epoch [5, 700] loss: 0.516\n",
      "Epoch [6, 100] loss: 0.413\n",
      "Epoch [6, 200] loss: 0.429\n",
      "Epoch [6, 300] loss: 0.436\n",
      "Epoch [6, 400] loss: 0.438\n",
      "Epoch [6, 500] loss: 0.463\n",
      "Epoch [6, 600] loss: 0.487\n",
      "Epoch [6, 700] loss: 0.461\n",
      "Epoch [7, 100] loss: 0.336\n",
      "Epoch [7, 200] loss: 0.354\n",
      "Epoch [7, 300] loss: 0.351\n",
      "Epoch [7, 400] loss: 0.385\n",
      "Epoch [7, 500] loss: 0.410\n",
      "Epoch [7, 600] loss: 0.380\n",
      "Epoch [7, 700] loss: 0.401\n",
      "Epoch [8, 100] loss: 0.304\n",
      "Epoch [8, 200] loss: 0.284\n",
      "Epoch [8, 300] loss: 0.330\n",
      "Epoch [8, 400] loss: 0.333\n",
      "Epoch [8, 500] loss: 0.341\n",
      "Epoch [8, 600] loss: 0.334\n",
      "Epoch [8, 700] loss: 0.337\n",
      "Epoch [9, 100] loss: 0.223\n",
      "Epoch [9, 200] loss: 0.246\n",
      "Epoch [9, 300] loss: 0.269\n",
      "Epoch [9, 400] loss: 0.278\n",
      "Epoch [9, 500] loss: 0.308\n",
      "Epoch [9, 600] loss: 0.344\n",
      "Epoch [9, 700] loss: 0.294\n",
      "Epoch [10, 100] loss: 0.196\n",
      "Epoch [10, 200] loss: 0.197\n",
      "Epoch [10, 300] loss: 0.225\n",
      "Epoch [10, 400] loss: 0.235\n",
      "Epoch [10, 500] loss: 0.249\n",
      "Epoch [10, 600] loss: 0.274\n",
      "Epoch [10, 700] loss: 0.253\n",
      "Accuracy on test set: 77.77%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "losses = []\n",
    "\n",
    "# Fine-tuning loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f'Epoch [{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            losses.append(running_loss / 100)\n",
    "            running_loss = 0.0\n",
    "\n",
    "# Test accuracy\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffcf3ad",
   "metadata": {},
   "source": [
    "#### Export vers ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a0ed96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18157/1095535220.py:4: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "dummy_input = torch.randn(1, 3, 32, 32).to(device)  # CIFAR-10 images are 32x32x3\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"resnet18_cifar10.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'},\n",
    "                 'output': {0: 'batch_size'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf46d348",
   "metadata": {},
   "source": [
    "#### Quantization du modÃ¨le ONNX et benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef1c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference time in milliseconds (lower is better):\n",
      "+----------------+----------------+----------------+-----------------+------------------+\n",
      "| Model          |   Batch Size 1 |   Batch Size 8 |   Batch Size 32 |   Batch Size 128 |\n",
      "+================+================+================+=================+==================+\n",
      "| PyTorch (ms)   |           4.34 |           7.42 |           18.9  |            41.48 |\n",
      "+----------------+----------------+----------------+-----------------+------------------+\n",
      "| ONNX FP32 (ms) |           1.1  |           2.58 |            7.51 |            23.33 |\n",
      "+----------------+----------------+----------------+-----------------+------------------+\n",
      "| ONNX INT8 (ms) |           1.55 |           8.74 |           26.16 |           117.96 |\n",
      "+----------------+----------------+----------------+-----------------+------------------+\n",
      "\n",
      "Comparing accuracy between models:\n",
      "\n",
      "Detailed comparison of first 5 samples:\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "True label: 3\n",
      "PyTorch prediction: 3 (confidence: 0.9312)\n",
      "ONNX prediction: 3 (confidence: 0.9312)\n",
      "Max difference in softmax outputs: 0.000000\n",
      "\n",
      "Sample 2:\n",
      "True label: 8\n",
      "PyTorch prediction: 8 (confidence: 0.9695)\n",
      "ONNX prediction: 8 (confidence: 0.9695)\n",
      "Max difference in softmax outputs: 0.000000\n",
      "\n",
      "Sample 3:\n",
      "True label: 8\n",
      "PyTorch prediction: 8 (confidence: 0.9519)\n",
      "ONNX prediction: 8 (confidence: 0.9519)\n",
      "Max difference in softmax outputs: 0.000000\n",
      "\n",
      "Sample 4:\n",
      "True label: 0\n",
      "PyTorch prediction: 0 (confidence: 0.5640)\n",
      "ONNX prediction: 0 (confidence: 0.5640)\n",
      "Max difference in softmax outputs: 0.000001\n",
      "\n",
      "Sample 5:\n",
      "True label: 6\n",
      "PyTorch prediction: 6 (confidence: 0.9867)\n",
      "ONNX prediction: 6 (confidence: 0.9867)\n",
      "Max difference in softmax outputs: 0.000000\n",
      "\n",
      "Overall Results:\n",
      "PyTorch Accuracy: 81.25%\n",
      "ONNX Runtime Accuracy: 81.25%\n",
      "\n",
      "Comparing accuracy with quantized model:\n",
      "\n",
      "Detailed comparison of first 5 samples:\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "True label: 3\n",
      "PyTorch prediction: 3 (confidence: 0.9312)\n",
      "ONNX prediction: 3 (confidence: 0.9404)\n",
      "Max difference in softmax outputs: 0.009160\n",
      "\n",
      "Sample 2:\n",
      "True label: 8\n",
      "PyTorch prediction: 8 (confidence: 0.9695)\n",
      "ONNX prediction: 8 (confidence: 0.9659)\n",
      "Max difference in softmax outputs: 0.003557\n",
      "\n",
      "Sample 3:\n",
      "True label: 8\n",
      "PyTorch prediction: 8 (confidence: 0.9519)\n",
      "ONNX prediction: 8 (confidence: 0.9424)\n",
      "Max difference in softmax outputs: 0.009488\n",
      "\n",
      "Sample 4:\n",
      "True label: 0\n",
      "PyTorch prediction: 0 (confidence: 0.5640)\n",
      "ONNX prediction: 0 (confidence: 0.5744)\n",
      "Max difference in softmax outputs: 0.011154\n",
      "\n",
      "Sample 5:\n",
      "True label: 6\n",
      "PyTorch prediction: 6 (confidence: 0.9867)\n",
      "ONNX prediction: 6 (confidence: 0.9865)\n",
      "Max difference in softmax outputs: 0.000762\n",
      "\n",
      "Overall Results:\n",
      "PyTorch Accuracy: 81.25%\n",
      "ONNX Runtime Accuracy: 81.25%\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Quantize the model\n",
    "model_fp32 = \"resnet18_cifar10.onnx\"\n",
    "model_quant = \"resnet18_cifar10_quantized.onnx\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=model_fp32,\n",
    "    model_output=model_quant,\n",
    "    weight_type=QuantType.QUInt8\n",
    ")\n",
    "\n",
    "model = model.cpu()\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Benchmark all models\n",
    "batch_sizes = [1, 8, 32, 128]\n",
    "input_shape = (3, 32, 32)  # CIFAR-10 dimensions\n",
    "pytorch_results = benchmark_pytorch_model(model, input_shape, batch_sizes)\n",
    "onnx_results = benchmark_onnx_model(model_fp32, input_shape, batch_sizes)\n",
    "quant_results = benchmark_onnx_model(model_quant, input_shape, batch_sizes)\n",
    "\n",
    "# Compare results\n",
    "headers = [\"Model\"] + [f\"Batch Size {bs}\" for bs in batch_sizes]\n",
    "data = [\n",
    "    [\"PyTorch (ms)\"] + [f\"{pytorch_results[bs]:.2f}\" for bs in batch_sizes],\n",
    "    [\"ONNX FP32 (ms)\"] + [f\"{onnx_results[bs]:.2f}\" for bs in batch_sizes],\n",
    "    [\"ONNX INT8 (ms)\"] + [f\"{quant_results[bs]:.2f}\" for bs in batch_sizes]\n",
    "]\n",
    "\n",
    "print(\"\\nInference time in milliseconds (lower is better):\")\n",
    "print(tabulate(data, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "print(\"\\nComparing accuracy between models:\")\n",
    "pytorch_softmax, onnx_softmax = compare_predictions(model, model_fp32, testloader)\n",
    "print(\"\\nComparing accuracy with quantized model:\")\n",
    "_, _ = compare_predictions(model, model_quant, testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
