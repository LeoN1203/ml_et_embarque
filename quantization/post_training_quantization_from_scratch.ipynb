{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "548aeae4",
   "metadata": {},
   "source": [
    "# Partie 8 : Post training quantization from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1729137e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation du device : cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Utilisation du device : {device}\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0a4975",
   "metadata": {},
   "source": [
    "### Partie 0 : Quantization naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbfa4d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "def train(model, loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(loader):.4f}\")\n",
    "    print(\"Entraînement terminé.\")\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def quantize(model):\n",
    "    \"\"\"\n",
    "    Crée une copie du modèle et convertit \"naïvement\" les poids \n",
    "    en arrondissant à l'entier le plus proche.\n",
    "    \"\"\"\n",
    "    quantized_model = copy.deepcopy(model)\n",
    "    quantized_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for param in quantized_model.parameters():\n",
    "            param.data = param.data.round() \n",
    "            \n",
    "    return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ae1be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Entraînement du modèle original (Float32) ---\n",
      "Epoch 1/5, Loss: 0.2573\n",
      "Epoch 2/5, Loss: 0.1125\n",
      "Epoch 3/5, Loss: 0.0783\n",
      "Epoch 4/5, Loss: 0.0597\n",
      "Epoch 5/5, Loss: 0.0479\n",
      "Entraînement terminé.\n",
      "\n",
      "Accuracy du modèle original (Float32): 97.67 %\n",
      "\n",
      "--- Quantification naïve du modèle ---\n",
      "Accuracy du modèle quantifié (Naïf): 9.80 %\n",
      "\n",
      "--- Comparaison ---\n",
      "Original (FP32) : 97.67 %\n",
      "Quantifié (Naïf) : 9.80 %\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Entraînement du modèle original (Float32) ---\")\n",
    "model_fp32 = MLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_fp32.parameters(), lr=0.001)\n",
    "\n",
    "train(model_fp32, train_loader, criterion, optimizer, epochs=5)\n",
    "\n",
    "original_accuracy = evaluate(model_fp32, test_loader)\n",
    "print(f\"\\nAccuracy du modèle original (Float32): {original_accuracy:.2f} %\")\n",
    "\n",
    "print(\"\\n--- Quantification naïve du modèle ---\")\n",
    "model_int_naive = quantize(model_fp32)\n",
    "\n",
    "quantized_accuracy = evaluate(model_int_naive, test_loader)\n",
    "print(f\"Accuracy du modèle quantifié (Naïf): {quantized_accuracy:.2f} %\")\n",
    "\n",
    "print(\"\\n--- Comparaison ---\")\n",
    "print(f\"Original (FP32) : {original_accuracy:.2f} %\")\n",
    "print(f\"Quantifié (Naïf) : {quantized_accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f2afaa",
   "metadata": {},
   "source": [
    "On remarque que la quantification naive a echouée.\n",
    "\n",
    "### Partie 1 : Quantization statique - weights only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e38501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_weights_single_range(model, num_bits=8):\n",
    "    \"\"\"\n",
    "    Quantifie les poids du modèle en utilisant une plage unique pour tous les paramètres\n",
    "    \"\"\"\n",
    "    quantized_model = copy.deepcopy(model)\n",
    "    quantized_model.eval()\n",
    "\n",
    "    global_min = float('inf')\n",
    "    global_max = float('-inf')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in quantized_model.parameters():\n",
    "            global_min = min(global_min, param.data.min().item())\n",
    "            global_max = max(global_max, param.data.max().item())\n",
    "    \n",
    "    scale = (global_max - global_min) / (2**num_bits - 1)\n",
    "    zero_point = -round(global_min / scale)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in quantized_model.parameters():\n",
    "            # Quantification\n",
    "            param.data = torch.round(param.data / scale + zero_point)\n",
    "            # Déquantification\n",
    "            param.data = (param.data - zero_point) * scale\n",
    "            \n",
    "    return quantized_model\n",
    "\n",
    "\n",
    "def quantize_weights_per_layer(model, num_bits=8):\n",
    "    \"\"\"\n",
    "    Quantifie les poids du modèle en utilisant une plage différente pour chaque couche\n",
    "    \"\"\"\n",
    "    quantized_model = copy.deepcopy(model)\n",
    "    quantized_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for name, module in quantized_model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                weight_min = module.weight.data.min().item()\n",
    "                weight_max = module.weight.data.max().item()\n",
    "                \n",
    "                scale = (weight_max - weight_min) / (2**num_bits - 1)\n",
    "                zero_point = -round(weight_min / scale)\n",
    "                \n",
    "                module.weight.data = torch.round(module.weight.data / scale + zero_point)\n",
    "                module.weight.data = (module.weight.data - zero_point) * scale\n",
    "                \n",
    "                if module.bias is not None:\n",
    "                    bias_min = module.bias.data.min().item()\n",
    "                    bias_max = module.bias.data.max().item()\n",
    "                    bias_scale = (bias_max - bias_min) / (2**num_bits - 1)\n",
    "                    bias_zero_point = -round(bias_min / bias_scale)\n",
    "                    \n",
    "                    module.bias.data = torch.round(module.bias.data / bias_scale + bias_zero_point)\n",
    "                    module.bias.data = (module.bias.data - bias_zero_point) * bias_scale\n",
    "    \n",
    "    return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbe2789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test de la quantification avec plage unique ---\n",
      "--- Test de la quantification avec plage par couche ---\n",
      "\n",
      "--- Comparaison des performances ---\n",
      "Original (FP32) : 97.67 %\n",
      "Quantifié (plage unique) : 97.68 %\n",
      "Quantifié (plage par couche) : 97.66 %\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Test de la quantification avec plage unique ---\")\n",
    "model_single_range = quantize_weights_single_range(model_fp32)\n",
    "single_range_accuracy = evaluate(model_single_range, test_loader)\n",
    "\n",
    "print(\"--- Test de la quantification avec plage par couche ---\")\n",
    "model_per_layer = quantize_weights_per_layer(model_fp32)\n",
    "per_layer_accuracy = evaluate(model_per_layer, test_loader)\n",
    "\n",
    "print(\"\\n--- Comparaison des performances ---\")\n",
    "print(f\"Original (FP32) : {original_accuracy:.2f} %\")\n",
    "print(f\"Quantifié (plage unique) : {single_range_accuracy:.2f} %\")\n",
    "print(f\"Quantifié (plage par couche) : {per_layer_accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf3d13",
   "metadata": {},
   "source": [
    "Cela semble mieux fonctionner.\n",
    "\n",
    "### Quantization des activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16b53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_activation_stats(model, loader, num_batches=100):\n",
    "    \"\"\"\n",
    "    Collecte les statistiques des activations sur un ensemble de calibration\n",
    "    \"\"\"\n",
    "    activation_stats = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def hook_fn(name):\n",
    "        def hook(module, input, output):\n",
    "            if name not in activation_stats:\n",
    "                activation_stats[name] = {\"min\": float('inf'), \"max\": float('-inf')}\n",
    "            activation_stats[name][\"min\"] = min(activation_stats[name][\"min\"], output.min().item())\n",
    "            activation_stats[name][\"max\"] = max(activation_stats[name][\"max\"], output.max().item())\n",
    "        return hook\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            hooks.append(module.register_forward_hook(hook_fn(name)))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, _) in enumerate(loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            inputs = inputs.to(device)\n",
    "            model(inputs)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return activation_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04750204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedMLP(nn.Module):\n",
    "    def __init__(self, original_model, num_bits=8, activation_stats=None):\n",
    "        super(QuantizedMLP, self).__init__()\n",
    "        self.num_bits = num_bits\n",
    "        self.activation_stats = activation_stats\n",
    "        \n",
    "        self.flatten = original_model.flatten\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for layer in original_model.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                weight_min = layer.weight.data.min().item()\n",
    "                weight_max = layer.weight.data.max().item()\n",
    "                weight_scale = (weight_max - weight_min) / (2**num_bits - 1)\n",
    "                weight_zero_point = -round(weight_min / weight_scale)\n",
    "                \n",
    "                quantized_weight = torch.round(layer.weight.data / weight_scale + weight_zero_point)\n",
    "                quantized_weight = (quantized_weight - weight_zero_point) * weight_scale\n",
    "                \n",
    "                new_layer = nn.Linear(layer.in_features, layer.out_features)\n",
    "                new_layer.weight.data = quantized_weight\n",
    "                \n",
    "                if layer.bias is not None:\n",
    "                    bias_min = layer.bias.data.min().item()\n",
    "                    bias_max = layer.bias.data.max().item()\n",
    "                    bias_scale = (bias_max - bias_min) / (2**num_bits - 1)\n",
    "                    bias_zero_point = -round(bias_min / bias_scale)\n",
    "                    \n",
    "                    quantized_bias = torch.round(layer.bias.data / bias_scale + bias_zero_point)\n",
    "                    quantized_bias = (quantized_bias - bias_zero_point) * bias_scale\n",
    "                    new_layer.bias.data = quantized_bias\n",
    "                \n",
    "                self.layers.append(new_layer)\n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                self.layers.append(layer)\n",
    "\n",
    "    def quantize_activation(self, x, stats):\n",
    "        \"\"\"Quantifie les activations en utilisant les statistiques collectées\"\"\"\n",
    "        min_val = stats[\"min\"]\n",
    "        max_val = stats[\"max\"]\n",
    "        scale = (max_val - min_val) / (2**self.num_bits - 1)\n",
    "        zero_point = -round(min_val / scale)\n",
    "        \n",
    "        x_quantized = torch.round(x / scale + zero_point)\n",
    "        x_dequantized = (x_quantized - zero_point) * scale\n",
    "        return x_dequantized\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.ReLU) and self.activation_stats is not None:\n",
    "                x = self.quantize_activation(x, self.activation_stats[f\"layers.{i}\"])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8003899",
   "metadata": {},
   "source": [
    "#### Collecte des statistiques d'activation et comparaisons finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa58e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecte des statistiques d'activation...\n",
      "\n",
      "--- Test de la quantification complète (poids + activations) ---\n",
      "\n",
      "--- Comparaison finale des performances ---\n",
      "Original (FP32) : 97.67 %\n",
      "Quantifié (poids uniquement) : 97.66 %\n",
      "Quantifié (poids + activations) : 97.64 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Collecte des statistiques d'activation...\")\n",
    "activation_stats = collect_activation_stats(model_fp32, train_loader, num_batches=50)\n",
    "\n",
    "print(\"\\n--- Test de la quantification complète (poids + activations) ---\")\n",
    "model_full_quant = QuantizedMLP(model_fp32, num_bits=8, activation_stats=activation_stats)\n",
    "full_quant_accuracy = evaluate(model_full_quant, test_loader)\n",
    "\n",
    "print(\"\\n--- Comparaison finale des performances ---\")\n",
    "print(f\"Original (FP32) : {original_accuracy:.2f} %\")\n",
    "print(f\"Quantifié (poids uniquement) : {per_layer_accuracy:.2f} %\")\n",
    "print(f\"Quantifié (poids + activations) : {full_quant_accuracy:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
